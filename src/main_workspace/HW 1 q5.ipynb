{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nathan Bush Q 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "print(dataset.data.shape)\n",
    "print(dataset.target.shape)\n",
    "old_target_variable = dataset.target\n",
    "dataset.target = 1 - old_target_variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used LLM formate print\n",
    "# Calculate mean and standard deviation for each metric\n",
    "def print_scores(results):\n",
    "    metrics = {\n",
    "        \"Accuracy\": results[\"test_accuracy\"],\n",
    "        \"F1 Score\": results[\"test_f1\"],\n",
    "        \"Precision\": results[\"test_precision\"],\n",
    "        \"Recall\": results[\"test_recall\"]\n",
    "    }\n",
    "    # Create a markdown table\n",
    "    print(\"| Metric    | Mean       |\")\n",
    "    print(\"|-----------|------------|\")\n",
    "    for metric, values in metrics.items():\n",
    "        mean = np.mean(values)\n",
    "        print(f\"| {metric:<9} | {mean:.4f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the dataset\n",
    "df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "# Extract the specified features\n",
    "df_selected = df[['worst concave points', 'worst radius', 'worst texture']]\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "scorers = (\"f1\",\"precision\",\"recall\",\"accuracy\")\n",
    "results = cross_validate(pipe, df_selected, dataset.target, scoring=scorers ,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Metric    | Mean       |\n",
      "|-----------|------------|\n",
      "| Accuracy  | 0.9649 \n",
      "| F1 Score  | 0.9522 \n",
      "| Precision | 0.9621 \n",
      "| Recall    | 0.9437 \n"
     ]
    }
   ],
   "source": [
    "print_scores(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Metric    | Mean       |\n",
      "|-----------|------------|\n",
      "| Accuracy  | 0.9807 \n",
      "| F1 Score  | 0.9736 \n",
      "| Precision | 0.9858 \n",
      "| Recall    | 0.9622 \n"
     ]
    }
   ],
   "source": [
    "#scale data first for better LogisticRegression\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "results = cross_validate(pipe, df, dataset.target, scoring=scorers ,cv=5)\n",
    "print_scores(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "  criterion: entropy\n",
      "  max_depth: 2\n",
      "  min_samples_split: 2\n",
      "\n",
      "Best Recall Score: 0.9347\n"
     ]
    }
   ],
   "source": [
    "# Set up the grid search with cross-validation\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [2, 3, 5, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10, 20]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator= DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=scorers,\n",
    "    refit='recall', \n",
    "    n_jobs=-1 # go faster\n",
    "    )\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(df, dataset.target)\n",
    "\n",
    "# Get the best parameters\n",
    "# Get the best parameters and scores\n",
    "best_params = grid_search.best_params_\n",
    "best_recall = grid_search.best_score_\n",
    "# Print the results\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest Recall Score: {best_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "  kneighborsclassifier__n_neighbors: 5\n",
      "  kneighborsclassifier__weights: uniform\n",
      "\n",
      "Best Recall Score: 0.9340\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'kneighborsclassifier__n_neighbors': [1, 3, 5, 8, 10, 15, 20, 50, 80], # change names for pipe normalize\n",
    "    'kneighborsclassifier__weights': ['uniform', 'distance']\n",
    "}\n",
    "pipe = make_pipeline(StandardScaler(), KNeighborsClassifier()) # should normalize for knn\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator= pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=scorers,\n",
    "    refit='recall', \n",
    "    n_jobs=-1 # go faster\n",
    "    )\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(df, dataset.target)\n",
    "\n",
    "# Get the best parameters\n",
    "# Get the best parameters and scores\n",
    "best_params = grid_search.best_params_\n",
    "best_recall = grid_search.best_score_\n",
    "# Print the results\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest Recall Score: {best_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5E\n",
    "As we are dealing with cancer and the target data has 'Malignant'= 1, and 'Benignâ€™=0 we want to make sure that we don't miss a cancerous tumor. If we miss a Malignant tumor (FN) someone may die. On the other hand FP are less costly, as futher testing would reveal the mistake. \n",
    "\n",
    "Recall is TP/(TP+FN), so it punishes FNs which is what we want. It does not punish FP but they cost less in this cass.\n",
    "\n",
    "In other words by using recall as our metric we are training our predicters to reduce the chance of someones cancer going undiagnosed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the grid search with cross-validation\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5, 6],\n",
    "    'min_samples_split': [2,3,5]\n",
    "}\n",
    "# use GradientBoostingClassifier\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=GradientBoostingClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=scorers,\n",
    "    refit='recall',\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(df, dataset.target)\n",
    "\n",
    "# Get the best parameters and scores\n",
    "best_params = grid_search.best_params_\n",
    "best_recall = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "  learning_rate: 0.1\n",
      "  max_depth: 5\n",
      "  min_samples_split: 5\n",
      "  n_estimators: 100\n",
      "\n",
      "Best Recall Score: 0.9576\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest Recall Score: {best_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can achieve significant gains compared to the simple logistic regression model in part a as to more complex model in part b did a bit better. In this case any small improvemnt would result in more lives saved so that numerically small improment is significant. As far as easy of use and perfermance logistic regression is the best of the models we used, so it is my prefered model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
