{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "# Specify the path to the CSV file\n",
    "file_path_train = \"data/train.csv\"\n",
    "file_path_test = \"data/test.csv\"\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path_train, delimiter=\",\", parse_dates=['host_since', 'first_review', 'last_review'])\n",
    "df_test = pd.read_csv(file_path_test, delimiter=\",\", parse_dates=['host_since', 'first_review', 'last_review'])\n",
    "\n",
    "# Drop the original price column and separate features and target\n",
    "y = df['price']\n",
    "X = df.drop(columns=['price'])\n",
    "ID_test = df_test[\"id\"]\n",
    "X_test = df_test.drop(columns=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '1 star', 'score': 0.9736103415489197}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\",device=0)\n",
    "#EX\n",
    "review = \"This house is terrible, do not go!\"\n",
    "result = classifier(review)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_batch_input(X_data, classifier):\n",
    "#     # Processing 'name' column\n",
    "#     batch_input = X_data[\"name\"].tolist()  # Convert to list for batch processing\n",
    "#     results = classifier(batch_input)  # Perform batch classification\n",
    "#     X_data['name_pred'] = [res['label'] for res in results]  # Assuming each result is a list of predictions\n",
    "#     X_data['name_score'] = [res['score'] for res in results]  # Adjust if classifier returns more\n",
    "\n",
    "#     # Processing 'description' column\n",
    "#     X_data['description'] = X_data['description'].astype(str)  # Ensure all are strings\n",
    "#     batch_input = X_data[\"description\"].tolist()  # Convert to list for batch processing\n",
    "#     results = classifier(batch_input)  # Perform batch classification\n",
    "#     X_data['description_pred'] = [res['label'] for res in results]  # Assuming each result is a list of predictions\n",
    "#     X_data['description_score'] = [res['score'] for res in results]  # Adjust if classifier returns more\n",
    "\n",
    "#     return X_data\n",
    "\n",
    "# # Usage\n",
    "# X = process_batch_input(X, classifier)\n",
    "# X_test = process_batch_input(X_test, classifier)\n",
    "# # Save predictions and scores for X\n",
    "# X[['name_pred','name_score','description_pred', 'description_score']].to_csv('data/my_data/X_description_predictions.csv', index=False)\n",
    "# # Save predictions and scores for X_test\n",
    "# X_test[['name_pred','name_score','description_pred', 'description_score']].to_csv('data/my_data/X_test_description_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract star level and create new columns\n",
    "def add_star_scores(df, col_pred, col_score, prefix):\n",
    "    for star in range(1, 6):  # Loop over star levels 1 to 5\n",
    "        star_label = f\"{star} stars\"\n",
    "        df[f\"{prefix}_{star}\"] = df.apply(\n",
    "            lambda row: row[col_score] if row[col_pred] == star_label else 0, axis=1\n",
    "        )\n",
    "    return df\n",
    "# Read the prediction files\n",
    "X_preds = pd.read_csv('data/my_data/X_description_predictions.csv')\n",
    "X_test_preds = pd.read_csv('data/my_data/X_test_description_predictions.csv')\n",
    "\n",
    "# Transform data for name and description\n",
    "X_preds = add_star_scores(X_preds, \"name_pred\", \"name_score\", \"name\")\n",
    "X_preds = add_star_scores(X_preds, \"description_pred\", \"description_score\", \"description\")\n",
    "# Drop original prediction columns\n",
    "X_preds = X_preds.drop(columns=[\"name_pred\", \"name_score\", \"description_pred\", \"description_score\"])\n",
    "\n",
    "# Transform data for name and description\n",
    "X_test_preds = add_star_scores(X_test_preds, \"name_pred\", \"name_score\", \"name\")\n",
    "X_test_preds = add_star_scores(X_test_preds, \"description_pred\", \"description_score\", \"description\")\n",
    "# Drop original prediction columns\n",
    "X_test_preds = X_test_preds.drop(columns=[\"name_pred\", \"name_score\", \"description_pred\", \"description_score\"])\n",
    "\n",
    "# Concatenate the predictions with X and X_test on axis 1 (column-wise)\n",
    "X = pd.concat([X, X_preds], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# import math\n",
    "# from collections import defaultdict\n",
    "# # Define candidate labels\n",
    "\n",
    "# # Function to classify multiple reviews for a single property\n",
    "# def classify_property_reviews(reviews_text, separator=\"---------------------------------\"):\n",
    "#     # Split concatenated reviews\n",
    "#     reviews = \"\"\n",
    "#     if pd.isna(reviews_text):        \n",
    "#         #print(reviews_text)\n",
    "#         reviews = [\"netrual\"]\n",
    "#     else: \n",
    "#         #print(reviews_text)\n",
    "#         reviews = reviews_text.split(separator)    \n",
    "#     # Classify reviews in batch using the pipeline\n",
    "#         reviews = reviews_text.split(separator)\n",
    "#         reviews = [review.strip() for review in reviews if review.strip()]  # Make sure each review is a clean, non-empty string\n",
    "    \n",
    "#     # Classify reviews in batch using the pipeline\n",
    "#     results = []\n",
    "#     for review in reviews:\n",
    "#         if ( len(review) >=512):\n",
    "#             review = review[0:510]\n",
    "#         result = classifier(review)\n",
    "#         results.append(result)\n",
    "\n",
    "#     # Step 1: Aggregate labels and scores\n",
    "#     aggregated_results = defaultdict(list)\n",
    "\n",
    "#     # Loop through the results and store the label and score\n",
    "#     for result in results:\n",
    "#         for entry in result:\n",
    "#             label = entry['label']\n",
    "#             score = entry['score']\n",
    "#             aggregated_results[label].append(score)\n",
    "\n",
    "#     # Step 2: Calculate the average score for each label\n",
    "#     final_results = {\n",
    "#         label: {'count': len(scores), 'average_score': sum(scores) / len(scores)}\n",
    "#         for label, scores in aggregated_results.items()\n",
    "#     }\n",
    "#     return final_results\n",
    "\n",
    "\n",
    "# # Convert X_test to Hugging Face Dataset format\n",
    "# # Assuming X_test is your original pandas DataFrame\n",
    "\n",
    "# # Apply the classification function to each review in a batch\n",
    "\n",
    "#     # Apply the classification function to each review\n",
    "# results = []\n",
    "# total_reviews = len(X['reviews'])  # Get the total number of reviews\n",
    "\n",
    "# for i, review in enumerate(X['reviews']):\n",
    "#     # Perform the classification\n",
    "#     results.append(classify_property_reviews(review))\n",
    "    \n",
    "#     # Print progress every 25 reviews as a percentage of the total\n",
    "#     if (i + 1) % 25 == 0:\n",
    "#         progress = (i + 1) / total_reviews * 100\n",
    "#         print(f\"Processed {i + 1} reviews out of {total_reviews} ({progress:.2f}%)\")\n",
    "\n",
    "\n",
    "# df_rows = []\n",
    "# for entry in results:\n",
    "#     row = {}\n",
    "#     for star, values in entry.items():\n",
    "#         row[f'{star}_count'] = values['count']\n",
    "#         row[f'{star}_average_score'] = values['average_score']\n",
    "#     df_rows.append(row)\n",
    "\n",
    "# # Create a DataFrame from the rows\n",
    "# df_rev = pd.DataFrame(df_rows)\n",
    "# # Write the DataFrame to a CSV file\n",
    "# df_rev.to_csv('data/my_data/classified_reviews_X.csv', index=False)  # index=False to avoid writing row indices\n",
    "# print(\"DataFrame written to 'src/main_workspace/Final_Project/data/my_data/classified_reviews.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file back into a DataFrame\n",
    "df_x_test_reviews= pd.read_csv('data/my_data/classified_reviews_X_test.csv')\n",
    "df_X_reviews= pd.read_csv('data/my_data/classified_reviews_X.csv')\n",
    "\n",
    "X_clean = X.copy()\n",
    "X_test_clean = X_test.copy()\n",
    "\n",
    "X_clean = pd.concat([X_clean, df_X_reviews], axis=1)\n",
    "X_test_clean = pd.concat([X_test_clean,df_x_test_reviews], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_amenities = [item for sublist in df['amenities'].to_numpy() for item in sublist]\n",
    "    \n",
    "#     # Count occurrences of each amenity\n",
    "# amenity_counts = Counter(all_amenities)\n",
    "# len(amenity_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_amenities(list_of_lists,n=10):\n",
    "    # Flatten the list of lists\n",
    "    all_amenities = [item for sublist in list_of_lists for item in sublist]\n",
    "    \n",
    "    # Count occurrences of each amenity\n",
    "    amenity_counts = Counter(all_amenities)\n",
    "    \n",
    "    # Get the top 10 most common amenities\n",
    "    top_n_amenities = amenity_counts.most_common(n)\n",
    "    \n",
    "    # Extract just the amenity names\n",
    "    top_amenity_names = [amenity for amenity,count in top_n_amenities]\n",
    "    \n",
    "    # Calculate the total number of amenities for each row\n",
    "    total_amenities_per_row = [len(sublist) for sublist in list_of_lists]\n",
    "    \n",
    "    return top_amenity_names, total_amenities_per_row\n",
    "\n",
    "# Convert the 'amenities' column from string format to a Python list\n",
    "X['amenities'] = X['amenities'].str.replace('[', '').str.replace(']', '').str.replace('\"', '').str.split(', ')\n",
    "X_test['amenities'] = X_test['amenities'].str.replace('[', '').str.replace(']', '').str.replace('\"', '').str.split(', ')\n",
    "# Get top 3000 amenities\n",
    "n = 3000\n",
    "top_amenities,total_amenities_per_row = get_top_amenities(X['amenities'].to_numpy(),n)\n",
    "top_amenities_test, total_amenities_per_row_test = get_top_amenities(X_test['amenities'].to_numpy(), n)\n",
    "\n",
    "# Convert datetime columns for X\n",
    "datetime_columns = ['host_since', 'first_review', 'last_review']\n",
    "for col in datetime_columns:\n",
    "    X[col] = pd.to_datetime(X[col], errors='coerce')\n",
    "    min_date = X[col].min()\n",
    "    X[f'{col}_numeric'] = (X[col] - min_date).dt.days\n",
    "    X[f'{col}_numeric'] = X[f'{col}_numeric'].replace({pd.NaT: np.nan})\n",
    "\n",
    "# Convert datetime columns for X_test\n",
    "for col in datetime_columns:\n",
    "    X_test[col] = pd.to_datetime(X_test[col], errors='coerce')\n",
    "    min_date = X_test[col].min()\n",
    "    X_test[f'{col}_numeric'] = (X_test[col] - min_date).dt.days\n",
    "    X_test[f'{col}_numeric'] = X_test[f'{col}_numeric'].replace({pd.NaT: np.nan})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=15696, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name (Type: object): Bed-Stuy 2 Bed/2 Bath - Renovated\n",
      "description (Type: object): Welcome to Bed-Stuy, Brooklyn! Our newly renovated brownstone has an apartment that is colorful and comfortable and entirely yours during your stay. Bed-Stuy is a friendly neighborhood with tree-lined streets and rows upon rows of traditional brownstones. We are close to many restaurants and coffee shops as well as perfectly located for exploring Brooklyn. Relax on the patio or take a quick trip into that other borough (Manhattan). We are a 5 minute walk to the Kingston/Throop C subway.\n",
      "property_type (Type: object): Entire rental unit\n",
      "price (Type: int64): 4\n",
      "neighbourhood_cleansed (Type: object): Bedford-Stuyvesant\n",
      "neighbourhood_group_cleansed (Type: object): Brooklyn\n",
      "latitude (Type: float64): 40.68456\n",
      "longitude (Type: float64): -73.93987\n",
      "host_since (Type: datetime64[ns]): 2015-05-23 00:00:00\n",
      "host_response_time (Type: object): within a day\n",
      "host_response_rate (Type: float64): 100.0\n",
      "host_acceptance_rate (Type: float64): 100.0\n",
      "host_is_superhost (Type: object): True\n",
      "host_listings_count (Type: float64): 2.0\n",
      "host_total_listings_count (Type: float64): 2.0\n",
      "host_verifications (Type: object): ['email', 'phone']\n",
      "host_has_profile_pic (Type: bool): True\n",
      "host_identity_verified (Type: bool): True\n",
      "calculated_host_listings_count (Type: int64): 1\n",
      "calculated_host_listings_count_entire_homes (Type: int64): 1\n",
      "calculated_host_listings_count_private_rooms (Type: int64): 0\n",
      "calculated_host_listings_count_shared_rooms (Type: int64): 0\n",
      "room_type (Type: object): Entire home/apt\n",
      "accommodates (Type: int64): 4\n",
      "bathrooms (Type: float64): 2.0\n",
      "bathrooms_text (Type: object): 2 baths\n",
      "bedrooms (Type: float64): 2.0\n",
      "beds (Type: float64): 2.0\n",
      "amenities (Type: object): [\"Dishwasher\", \"Single level home\", \"Stove\", \"Smoke alarm\", \"49 inch HDTV with Amazon Prime Video, Apple TV, Disney+, HBO Max, Hulu, Netflix, Roku\", \"Central air conditioning\", \"Cooking basics\", \"Heating\", \"Private patio or balcony\", \"Exterior security cameras on property\", \"Refrigerator\", \"Long term stays allowed\", \"Kitchen\", \"Hair dryer\", \"Hot water\", \"Free street parking\", \"Iron\", \"Private entrance\", \"Fire extinguisher\", \"Microwave\", \"Essentials\", \"Smart lock\", \"Carbon monoxide alarm\", \"Hangers\", \"Shampoo\", \"Dishes and silverware\", \"Oven\", \"Bathtub\", \"Self check-in\", \"First aid kit\", \"Coffee maker\", \"Wifi\"]\n",
      "has_availability (Type: object): True\n",
      "availability_30 (Type: int64): 12\n",
      "availability_60 (Type: int64): 42\n",
      "availability_90 (Type: int64): 70\n",
      "availability_365 (Type: int64): 70\n",
      "instant_bookable (Type: bool): False\n",
      "minimum_nights (Type: int64): 30\n",
      "maximum_nights (Type: int64): 1125\n",
      "number_of_reviews (Type: int64): 34\n",
      "number_of_reviews_ltm (Type: int64): 5\n",
      "number_of_reviews_l30d (Type: int64): 1\n",
      "first_review (Type: datetime64[ns]): 2019-04-28 00:00:00\n",
      "last_review (Type: datetime64[ns]): 2024-08-10 00:00:00\n",
      "review_scores_rating (Type: float64): 5.0\n",
      "review_scores_accuracy (Type: float64): 5.0\n",
      "review_scores_cleanliness (Type: float64): 4.97\n",
      "review_scores_checkin (Type: float64): 5.0\n",
      "review_scores_communication (Type: float64): 5.0\n",
      "review_scores_location (Type: float64): 4.71\n",
      "review_scores_value (Type: float64): 4.94\n",
      "reviews_per_month (Type: float64): 0.52\n",
      "reviews (Type: object): Barry's place was perfect. It was cute, modern, spacious and clean. Check in and out was super easy. Barry even stopped by on the first day to say hello and bring my daughter cupcakes. He knew we were in town for her birthday. That was a very sweet gesture. Zero complaints. I would highly recommend staying here.\n",
      "---------------------------------\n",
      "Definitely highly recommended. I was one of his first guests, and he asked me if there’s any suggestions i could give him for improvements after staying, but there’s really none i could think of. The place is that perfect - very clean, well organized, cozy yet modern, and has everything you need. The place was really lovely, and very praiseworthy, yet the best part of staying here, i think, was the host. He always checked out if i’m doing ok and offered help whenever i need. It’s like you get to enjoy the perfect privacy as you use the whole floor on your own, but having a local person who can always help you out. I’ve used quite a list of air bnbs around the world and in the city and this one was definitely one of the best.\n",
      "---------------------------------\n",
      "Beautiful apartment in the Brooklyn area.  Easy walk to the subway.  Great communication, easy check -in.  Super clean, chic apartment with all amenities you would expect.  Thanks for hosting us! We would stay here again :)\n",
      "---------------------------------\n",
      "Barry's apartment was beautiful and looks just like the pictures. It was clean and fresh with great amenities.Check in and out was simple. It was located less than 10 minutes walk from the C train which takes you straight to Penn Station in Manhattan.I would definitely recommend!!\n",
      "---------------------------------\n",
      "We came to NYC to celebrate anniversaries with friends. Barry’s place was as beautiful as pictured. He is a great host, very concerned about our comfort, enjoyment of the space & quick to respond to messages. It was also very thoughtful that he added some special touches for our anniversary. We appreciated the flexibility with check in time (obviously subject to other bookings). We would definitely recommend Barry’s place!\n",
      "---------------------------------\n",
      "Barry was an amazing host. Check in/out was smooth. The apartment was clean, comfortable and nicely decorated. Loved the hand written note on the back of a 40’s photo of the brownstone! Subway was a short walk away. Would love to stay here again.\n",
      "---------------------------------\n",
      "El departamento de Barry rebasó las expectativas que teníamos. Tuvo muy lindos detalles con nosotros, como chocolates, refrescos, cervezas y hasta una botella de jugo de manzana. La decoración sencilla, pero hermosa, con fotografías en paredes del Brooklyn Brige en diferentes ángulos. Muy limpio y ordenado. Hacer el checking fué super sencillo y, como nuestro vuelo salía tarde, amablemente nos resguardo nuestras maletas para aprovechar el día. La ubicación excelente, en un área muy tranquila, rodeada de tiendas de suministros y a unas cuantas cuadras del metro A y C. Lo recomiendo ampliamente. Con gusto me hospedaría de nuevo ahí.\n",
      "---------------------------------\n",
      "虽然是老房子，但是新装修的，挺干净的，设备也基本齐全。门口路上虽然可以停车，但停车位比较紧张。我们2对夫妇出行都是座地铁的。附近超市和饭店都不少，生活便利程度高。房门是数码锁，自主入住，房东回复也非常快。\n",
      "---------------------------------\n",
      "Barry’s place was absolutely wonderful. His place is sparkling clean and a great size. It’s the quietest sleep I’ve ever had in NYC. The walk to the subway was super close. Barry was amazing to deal with and very accommodating. The best host I’ve ever dealt with.\n",
      "---------------------------------\n",
      "Lugar fantástico, anfitrião nos deixou a vontade ,  sempre disponível  quando precisamos.Casa estava muito limpa, organizada e com todos utensílios necessários para estadia.Pena ter apenas 5 estrelas para avaliar, merece mais\n",
      "---------------------------------\n",
      "Great apartment in a great location. We loved Brooklyn and, in particular, Bed-Stuy. Thank you Barry for being a fantastic host.\n",
      "---------------------------------\n",
      "Je recommande cet appartement les yeux fermés. Il répondait à nos attentes, et même plus encore. Très fonctionnel, décoré avec goût, et très facile d’accès. Barry est également très à l’écoute. Séjour parfait.\n",
      "---------------------------------\n",
      "My two sisters, my mom and I stayed in Barry's house in Bed-Stuy for my mom's birthday. We absolutely loved the apartment and were very comfortable there. The space is very nice and we had every amenity we needed. Barry was a wonderful host who was always very quick to respond and available to answer questions or replace something. It was a close walk to the subway and very easy to get to downtown Manhattan where we spend a lot of our time. We did hang out in the Bed-Stuy neighborhood as well which we enjoyed a lot. We all loved Grandchamps which is a great Haitian cafe a short walk from the apartment!\n",
      "---------------------------------\n",
      "If you ever desired the real NYC experience in an actual historic brownstone, in a quiet & charming neighborhood, then Barry’s place is for you! From extra welcoming touches to an attractive  & welcoming space, this is a wonderful stay. We definitely would stay there again.\n",
      "---------------------------------\n",
      "Lovely apartment and exceptional host\n",
      "---------------------------------\n",
      "Wir hatten eine wunderbare Zeit zu viert in dem Apartment. Die Wohnung ist genauso schön wie auf den Bildern und sehr gut ausgestattet. Wir waren eine ganze Woche in Barry's tollen Wohnung und haben NYC erkundet. Wir waren als 2 Pärchen dort und es war wirklich perfekt dafür. Morgens konnte man schön frühstücken und abends auch mal gemütlich auf dem Sofa entspannen und dabei etwas kochen. (Die Erkundung von NYC kann etwas anstrengend werden). Auch die Kommunikation mit Barry hat hervorragend geklappt! Vielen Dank für die schöne Zeit.\n",
      "---------------------------------\n",
      "Our stay at Barry's apartment was amazing. Barry is a super host and the apartment is great. We were a group of 4 and were able to feel at home. Barry quickly responded and helped us with any questions we had.The apartment is very well decorated and cozy. Near the subway and with some grocery stores nearby too. We were very happy to find this Airbnb.\n",
      "---------------------------------\n",
      "It was a great stay at the apartment!, beautiful details and great stuff in the kitchen... Beautiful bathrooms with everything you need. Cosy bedrooms with a selection of pillows. The neighborhood is really beautiful and authentically for New York.Barry is a great host and always there for you if you need anything during your stay or would like to know anything before your trip.\n",
      "---------------------------------\n",
      "Brooklyn is a few subway stops from Manhattan and Barry's place is just a 7 minute walk from the subway.Best of all - Barry's place is great (we are a family of 4 and it was perfect). Barry is responsive and attentive - you won't do much better.Well done Barry - you deserve to be full every night.RegardsBruce\n",
      "---------------------------------\n",
      "Our stay at Barry's place was great! Check-in was a piece of cake and Barry provided us with everything we could need and was great to communicate with. The house is about 10 minutes from the subway, which was perfect, even on cold evenings! We had an infant with us and the space worked well for our needs, it was a plus to be ground level so no need to carry baby upstairs! It was cozy and not too much storage space, but was beautiful. Amazing value overall! I would stay here again, without hesitation!\n",
      "---------------------------------\n",
      "This was the best Airbnb we have ever stayed in. The location is perfect. It’s safe and there is a lot to do around the area. It’s a close walk to the subway. The apartment is newly renovated and very clean. It has every appliance we could ever need. The host Barry was exceptional. He always made sure we were okay and went above and beyond to make sure we enjoyed our stay. Word can’t describe how exceptional Barry his! Thank you Barry if you’re reading this . Definitely going to try to use this place every time i need to find a place in Brooklyn. If you’re looking for a place in Brooklyn, stay here, you won’t regret it. Literally have 0 complaints and I’m pretty picky.\n",
      "---------------------------------\n",
      "If I could give more than 5 stars to Barry I would! I stayed in Barry’s garden level apartment for a month with a friend. It’s an exceptionably comfortable space for a long term Airbnb. The apartment is as pictured- 2 bedrooms, 2 baths and a large kitchen/living room. However, what makes this rental stand out is all the attention to details - the furnishings are cozy and high quality, and the kitchen is stocked with everything you need to feel at home. I also really appreciated the Bluetooth speaker and the TV with lots of streaming services. Barry was the most attentive and kind host I’ve ever had on Airbnb. He lives above the unit and usually responds within a couple minutes to any messages. When we arrived he had snacks/coffee/tea waiting for us. At one point I was cooking a more elaborate dinner and he responded immediately with some ad hoc requests for baking supplies. The location of the rental could also not be any better. It’s situated on a quiet, safe tree-lined street. I loved walking around the neighborhood looking at all the historic brownstones. There’s an abundance of local cafes, bars, and restaurants within a couple blocks. If you need to commute into Manhattan or downtown Brooklyn, the A/C line is nearby. Wish I could’ve stayed longer!\n",
      "---------------------------------\n",
      "This is the best airbnb I've stayed at, apartment is great (clean, spacious, has anything you need, close to subway) but more importantly Barry is the best host you can have. He's always available for communication, I was just moving from Chile to NYC and he made it so easy for me to transition, helping me and my mom with anything we needed. Dont look any further, you wont regret it!\n",
      "---------------------------------\n",
      "BARRY'S PLACE IS THE BEST LONG STAY I'VE EVER HAD WITH AN AIRBNB! Here's why:- Location: The apartment is located on the ground level (no stairs!!) in a quiet tree-lined street in a well-maintained secure, brownstone. Despite it being quiet, the subway (blue line) is a 5-minute walk away that will take you to Manhattan in 20 minutes. There are also tons of restaurants, grocery stores, shopping, laundromats, and other conveniences within walking distance. In addition to the subway, there are numerous bus lines that run nearby which will also get you where you need to go in Brooklyn!-Barry: He is the kindest, most caring host ever. He lives upstairs and is generally available for any questions or concerns you may have. As a host myself, he has made me want to improve how I work with guests!! He had snacks and a wonderful bottle of Prosecco waiting for us when we checked in. His communication is top-notch and I felt very comfortable booking a 6-week stay with him after all the pre-booking communication we had. During our stay, he checked in on us frequently and made sure we had everything we needed - including additional trash bags and a vacuum cleaner on request.-The space: The apartment is exactly as described in the listing. The beds in both bedrooms were super comfortable, appliances in the kitchen were modern and worked great for storing food and cooking, both bathrooms were spacious and functional, well-appointed living room with a nice sized smart TV, Wifi worked great throughout the apartment and I had no issues connecting to video calls. Now that I think about it, the kitchen was well stocked with everything you will need to cook a large meal and even host people - there were mixing bowls, a full-sized fridge, champagne flutes, coffee pots, utensils, pots & pans, etc. The only thing that this place was missing is a washer/dryer but this is a luxury in NYC and is totally understandable. There are 2 or 3 laundromats nearby that make up for this.I could go on and on about this place but I'll end it here. Bottom line is that if you need a comfortable place to stay for a long or short stay in New York City, this place cannot be beaten. I will definitely be back if I'm ever in the city again!!\n",
      "---------------------------------\n",
      "You won’t find any place more clean and you won’t find a homeowner more caring and helpful , and as rare as that is in general, it is extremely rare in Brooklyn ! The owners communicate consistently and respond to you extremely fast.  Thé street is very safe and the area is very convenient. It was definitely the hospitality for me !\n",
      "---------------------------------\n",
      "Great experience! The house was super nice and clean. House owner is a very friendly person that helps us a lot. We felt like living in home. The location is very close to the subway and there is a nice supermarket nearby as well.\n",
      "---------------------------------\n",
      "Saying that Barry is a super-host is an understatement! My job had me travelling a lot over the years and I stayed in so many places I could not even remember many of them, but for sure I will remember this one for a long time. Barry is simply the next level above super-host. He communicates very well, does his best to make you comfortable and helps you with whatever you need. And I really mean whatever you need! The place is exactly as described and showed in the pictures. I found it sparkling clean with a lot of things to get us started (vinegar and oil, multiple types, spices of all kind, fruits and snacks, water, tea and good quality coffee, and so many others). I am not mentioning the basics (linen, soap, towels, kitchenware, etc) - everything you need is there. The appliances are of very good quality (microwave, stove, fridge, dishwasher) and working perfectly. I loved the coffee filter, my every morning friend :) Even if you stay for long time and want to cook inside, or you just want to order and enjoy the meal with your party, all you could need is there.What is not mentioned in description and I found particularly nice is the AC / climate control. It is all automated; you can play with a lot of functions or you can just set the temperature you want and it takes care of everything. The air is distributed through the walls and you can adjust the vents if you want to decrease the flow in a particular place. The system makes your stay so comfortable regardless of the weather outside, cold or hot, wet or dry. It works really well and quietly, you don't hear it at all, even in the night. The location is on a nice and quiet street, several blocks away from the metro (line C). Exactly on the street there is no bus line and there is little traffic, which makes it particularly quiet. Around the block there are three bus lines, at the left corner of the block there is a bus going north (43), at the right corner a bus going south (15, also reaching the AirTrain for the JFK airport) and exactly behind the block a bus going both ways (east - west, number 26), getting all the way to downtown Brooklyn and Brooklyn Heights. From the last stop Dumbo is only 5 minutes walk away.If you got to read this review so far, it means the place is right for you and you can stop looking further. Enjoy your stay !\n",
      "---------------------------------\n",
      "Barry is a super host. I'm very happy to have chosen his apartment for our first month in NY. His apartment is exquisitely decorated and extremely clean. I would recommend this apartment to all my friends coming to visit NY.  The house is located in a nice and calm street and there are many supermarkets in the area. Metro is very close and it was very easy to move around from this location. Barry and his partner are very friendly and supportive especially if you're new to the city and you need advice.  Barry and his apartment are above and beyond! Thank you so much for hosting us in your amazing apartment.\n",
      "---------------------------------\n",
      "Barry was an amazing host!! He was very  friendly and accessible had great suggestions for food around the area.Whenever we saw him in the neighborhood he would say hi and at times stop and chat. He made us feel right at home and welcomed into the neighborhood. When we had issues w packages being delivered he suggested we have them place in the outdoor closet.\n",
      "---------------------------------\n",
      "barrys place is nice, clean and nicely located in bed stuy.barry has been a fantastic host. his local recommendations were spot on and he was very responsive and accomodative of requests.\n",
      "---------------------------------\n",
      "Best Airbnb I've ever stayed in! Barry's place was cozy, neat, clean, and well-stocked with everything I needed for a 6-week stay. Barry consistently went above and beyond as a host -- he was extremely responsive and checked in periodically to make sure we had everything we needed. Totally exceeded my expectations!\n",
      "---------------------------------\n",
      "很开心的一次旅行！房屋跟图片一样：温馨舒适！干净整洁！房东非常棒：热情、专业、负责和细心！让我们在异乡也感受到了家的温暖！\n",
      "---------------------------------\n",
      "The host Berry was very welcoming and helpful. He responded quickly to our questions and provided useful information about the area.The check-in process was simple and smooth thanks to the clear instructions provided.The apartment was clean, but there was some dirt in areas like behind the bed and on the shelves. It perfectly matched the photos and the description in the listing. The bed was comfortable and the furnishings were modern and functional. We greatly appreciated the well-equipped kitchen and the fast Wi-Fi, although each time we cooked, the smoke alarm risked going on.The terrace with the two chairs was much appreciated, as was the living room with the TV.The neighborhood was quiet and safe, with various shops of all kinds (restaurant, market, laundry...). The metro stop was only a 7-minute walk away, making it easy to get around the city.We felt at home and appreciated all the comforts offered, such as the two bedrooms and the two bathrooms.Again many thanks to Berry!\n",
      "---------------------------------\n",
      "Barry was attentive to all our needs. The place is comfortable and just what we needed for our 4 week stay.\n"
     ]
    }
   ],
   "source": [
    "# Print the type and first row value of each column\n",
    "for col in df.columns:\n",
    "    print(f\"{col} (Type: {df[col].dtype}): {df[col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50575/1146424666.py:26: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_clean['host_is_superhost'] = X_clean['host_is_superhost'].fillna(False).astype(bool)\n",
      "/tmp/ipykernel_50575/1146424666.py:27: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_clean['has_availability'] = X_clean['has_availability'].fillna(False).astype(bool)\n"
     ]
    }
   ],
   "source": [
    "# Separate numeric and other columns\n",
    "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "other_cols = X.select_dtypes(include=['object']).columns\n",
    "#X_clean = X.copy()\n",
    "\n",
    "# Create one-hot encoded columns for the top amenities in a vectorized way\n",
    "amenities_df = pd.DataFrame(\n",
    "    {f'has_{amenity.lower().replace(\" \", \"_\")}': X_clean['amenities'].apply(lambda x: int(amenity in x)) \n",
    "     for amenity in top_amenities}\n",
    ")\n",
    "\n",
    "# Concatenate the new columns with the original DataFrame\n",
    "X_clean = pd.concat([X_clean, amenities_df], axis=1)\n",
    "    \n",
    "X_clean = X_clean.drop(columns=[\"name\",\"description\",\"reviews\",\"amenities\",\"bathrooms_text\",'host_since', 'first_review', 'last_review'])\n",
    "\n",
    "# make new row for total_amenities\n",
    "X_clean['total_amenities'] = total_amenities_per_row\n",
    "\n",
    "\n",
    "#might remove neighbourhood_cleansed as it has a lot\n",
    "cat_columns = ['property_type', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed', \"host_verifications\",\"host_response_time\",\"room_type\"]\n",
    "X_clean[cat_columns] = X_clean[cat_columns].astype('category')\n",
    "\n",
    "#fix boolean data\n",
    "X_clean['host_is_superhost'] = X_clean['host_is_superhost'].fillna(False).astype(bool)\n",
    "X_clean['has_availability'] = X_clean['has_availability'].fillna(False).astype(bool)\n",
    "\n",
    "\n",
    "# Scale only the numeric columns\n",
    "# scaler = StandardScaler()\n",
    "# X_clean[numeric_cols] = scaler.fit_transform(X_clean[numeric_cols])\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "#X_clean = pd.get_dummies(X_clean, columns=cat_columns, drop_first=True)\n",
    "\n",
    "# converting a categorical column to integer labels\n",
    "for col in cat_columns:  # replace with your actual categorical columns\n",
    "    X_clean[col] = LabelEncoder().fit_transform(X_clean[col])\n",
    "categorical_features = [X_clean.columns.get_loc(col) for col in cat_columns]\n",
    "\n",
    "# Function to clean column names\n",
    "def clean_column_name(col_name):\n",
    "    return col_name.replace('[', '').replace(']', '').replace('<', '')\n",
    "# Apply the cleaning function to all column names\n",
    "X_clean.columns = X_clean.columns.map(clean_column_name)\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_clean, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50575/3842504247.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_clean['host_is_superhost'] = X_test_clean['host_is_superhost'].fillna(False).astype(bool)\n",
      "/tmp/ipykernel_50575/3842504247.py:26: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_clean['has_availability'] = X_test_clean['has_availability'].fillna(False).astype(bool)\n"
     ]
    }
   ],
   "source": [
    "# Separate numeric and other columns for X_test\n",
    "numeric_cols_test = X_test.select_dtypes(include=['float64', 'int64']).columns\n",
    "other_cols_test = X_test.select_dtypes(include=['object']).columns\n",
    "#X_test_clean = X_test.copy()\n",
    "\n",
    "# Create one-hot encoded columns for the top amenities in a vectorized way for X_test\n",
    "amenities_df_test = pd.DataFrame(\n",
    "    {f'has_{amenity.lower().replace(\" \", \"_\")}': X_test_clean['amenities'].apply(lambda x: int(amenity in x)) \n",
    "     for amenity in top_amenities}\n",
    ")\n",
    "\n",
    "# Concatenate the new columns with the original DataFrame for X_test\n",
    "X_test_clean = pd.concat([X_test_clean, amenities_df_test], axis=1)\n",
    "\n",
    "X_test_clean = X_test_clean.drop(columns=[\"name\",\"description\",\"reviews\",\"amenities\",\"bathrooms_text\",'host_since', 'first_review', 'last_review'])\n",
    "\n",
    "# Make new row for total_amenities for X_test\n",
    "X_test_clean['total_amenities'] = total_amenities_per_row_test\n",
    "\n",
    "# might remove neighbourhood_cleansed as it has a lot of categories\n",
    "cat_columns_test = ['property_type', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed', \"host_verifications\",\"host_response_time\",\"room_type\"]\n",
    "X_test_clean[cat_columns_test] = X_test_clean[cat_columns_test].astype('category')\n",
    "\n",
    "# Fix boolean data for X_test\n",
    "X_test_clean['host_is_superhost'] = X_test_clean['host_is_superhost'].fillna(False).astype(bool)\n",
    "X_test_clean['has_availability'] = X_test_clean['has_availability'].fillna(False).astype(bool)\n",
    "\n",
    "# One-hot encode categorical columns if needed\n",
    "# X_test_clean = pd.get_dummies(X_test_clean, columns=cat_columns_test, drop_first=True)\n",
    "\n",
    "# Converting a categorical column to integer labels for X_test\n",
    "for col in cat_columns_test:\n",
    "    X_test_clean[col] = LabelEncoder().fit_transform(X_test_clean[col])\n",
    "categorical_features_test = [X_test_clean.columns.get_loc(col) for col in cat_columns_test]\n",
    "\n",
    "# Function to clean column names for X_test\n",
    "def clean_column_name(col_name):\n",
    "    return col_name.replace('[', '').replace(']', '').replace('<', '')\n",
    "\n",
    "# Apply the cleaning function to all column names in X_test\n",
    "X_test_clean.columns = X_test_clean.columns.map(clean_column_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_clean.columns)\n",
    "# for col in X_clean.columns:\n",
    "#     print(f\"{col} (Type: {X_clean[col].dtype}): {X_clean[col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[ True False]\n"
     ]
    }
   ],
   "source": [
    "unique_categories = df['host_identity_verified'].unique()\n",
    "# Verify the result\n",
    "print(len(unique_categories))\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_columns = X_clean.select_dtypes(include=['object']).columns\n",
    "# print(\"Columns of type 'object' in X_clean:\")\n",
    "# for col in object_columns:\n",
    "#     print(col)\n",
    "# object_columns = X_clean.select_dtypes(include=['datetime']).columns\n",
    "# print(\"Columns of type 'time' in X_clean:\")\n",
    "# for col in object_columns:\n",
    "#     print(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8070154182853748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize and train HistGradientBoostingClassifier or HistGradientBoostingRegressor\n",
    "# model = HistGradientBoostingClassifier(categorical_features=categorical_features, random_state=42)\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.01],\n",
    "#     'max_iter': [200],\n",
    "#     'max_depth': [9],\n",
    "#     'l2_regularization': [0.1, 0.2],\n",
    "#     'min_samples_leaf': [3]\n",
    "# }\n",
    "# # Set up the grid search\n",
    "# grid_search = GridSearchCV(model, param_grid, cv=5,verbose=3, n_jobs=-1)\n",
    "\n",
    "# # Fit the model\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best parameters\n",
    "# print(grid_search.best_params_)\n",
    "# # Use the best model from the grid search to make predictions\n",
    "# best_model = grid_search.best_estimator_\n",
    "# #y_pred = np.round(best_model.predict(X_val))\n",
    "# y_pred = (best_model.predict(X_val))\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "# class_report = classification_report(y_val, y_pred)\n",
    "\n",
    "\n",
    "# # Calculate RMSE by treating the quantile labels as numeric values\n",
    "# rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "# print(\"Root Mean Squared Error on validation set:\", rmse)\n",
    "\n",
    "# print(\"Accuracy on validation set:\", accuracy)\n",
    "# print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "# print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9132778509966281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize and train HistGradientBoostingClassifier\n",
    "# # Define the parameter grid\n",
    "# params = {'l2_regularization': 0.2,'learning_rate': 0.1, 'max_depth': 9, 'max_iter': 100, 'min_samples_leaf': 5}\n",
    "\n",
    "# model = HistGradientBoostingClassifier(categorical_features=categorical_features, random_state=42, **params)\n",
    "\n",
    "# # Fit the model\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# #y_pred = np.round(best_model.predict(X_val))\n",
    "# y_pred = (model.predict(X_val))\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "# class_report = classification_report(y_val, y_pred)\n",
    "\n",
    "\n",
    "# # Calculate RMSE by treating the quantile labels as numeric values\n",
    "# rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "# print(\"Root Mean Squared Error on validation set:\", rmse)\n",
    "\n",
    "# print(\"Accuracy on validation set:\", accuracy)\n",
    "# print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "# print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.75354787167402880."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7524905568952746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Root Mean Squared Error on validation set: 0.7411902976461854\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 0.9987261146496815\n",
      "Root Mean Squared Error on validation set: 0.03569153051241248\n",
      "Accuracy on training set: 0.9980\n",
      "Root Mean Squared Error on training set: 0.04462151904374463\n",
      "\n",
      "Classification Report on validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       540\n",
      "           1       1.00      1.00      1.00       524\n",
      "           2       1.00      1.00      1.00       513\n",
      "           3       1.00      1.00      1.00       556\n",
      "           4       1.00      1.00      1.00       463\n",
      "           5       1.00      1.00      1.00       544\n",
      "\n",
      "    accuracy                           1.00      3140\n",
      "   macro avg       1.00      1.00      1.00      3140\n",
      "weighted avg       1.00      1.00      1.00      3140\n",
      "\n",
      "\n",
      "Top 10 Important Features:\n",
      "latitude: 20127.0\n",
      "longitude: 19399.0\n",
      "neighbourhood_cleansed: 13925.0\n",
      "availability_365: 13612.0\n",
      "description_5: 13540.0\n",
      "name_5: 12629.0\n",
      "host_acceptance_rate: 12120.0\n",
      "total_amenities: 11981.0\n",
      "5 stars_average_score: 11979.0\n",
      "4 stars_average_score: 9943.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the model with custom parameters\n",
    "# params = {\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 9,\n",
    "#     'min_child_weight': 3,\n",
    "#     'reg_lambda': 1.0,\n",
    "#     'reg_alpha': 0.1,\n",
    "#     'n_estimators': 1000,\n",
    "#     'subsample': 0.8,\n",
    "#     'colsample_bytree': 0.8,\n",
    "#     'objective': 'reg:squarederror',  # For regression\n",
    "#     'eval_metric': 'rmse',            # RMSE as evaluation metric\n",
    "# }\n",
    "\n",
    "\n",
    "#Best_Parameters =  {'learning_rate': 0.020534279384947673, 'max_depth': 9, 'n_estimators': 1800, 'subsample': 0.6817057463318822, 'colsample_bytree': 0.7324724625794548, 'reg_lambda': 0.2145407717059405, 'reg_alpha': 0.7071638502266191, 'min_child_weight': 1,'random_state': 1  }\n",
    "Best_Parameters =  {'learning_rate': 0.020534279384947673, 'max_depth': 9, 'n_estimators': 1800, 'subsample': 1, 'colsample_bytree': 0.7324724625794548, 'reg_lambda': 0.2145407717059405, 'reg_alpha': 0.7071638502266191, 'min_child_weight': 1,'random_state': 1}\n",
    "\n",
    "#est_Parameters = {'max_depth': 9, 'n_estimators': 1800, 'learning_rate': 0.08748281247944359, 'subsample': 0.7995986950841062, 'colsample_bytree': 0.7148792079582755, 'reg_lambda': 0.9949385383449679, 'reg_alpha': 0.7615649809168463, 'min_child_weight': 3}\n",
    "\n",
    "# Initialize the XGBRegressor with the specified parameters\n",
    "model = xgb.XGBRegressor(**Best_Parameters)\n",
    "\n",
    "#model = xgb.XGBRegressor(objective=\"reg:squarederror\", eval_metric=\"rmse\")\n",
    "\n",
    "# Initialize and train XGBClassifier model\n",
    "#model.fit(X_train, y_train)\n",
    "model.fit(X_clean, y)\n",
    "\n",
    "# Predictions on validation set\n",
    "y_pred = y_pred_bounded = np.clip(np.round(model.predict(X_val)), 0, 5)\n",
    "\n",
    "# Calculate accuracy and RMSE on validation set\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy on validation set:\", accuracy)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(\"Root Mean Squared Error on validation set:\", rmse)\n",
    "\n",
    "# Predictions on training set\n",
    "y_pred_train = np.clip(np.round(model.predict(X_train)), 0, 5)\n",
    "\n",
    "# Calculate accuracy and RMSE on training set\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Accuracy on training set: {train_accuracy:.4f}\")\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "print(\"Root Mean Squared Error on training set:\", train_rmse)\n",
    "\n",
    "# Classification report on validation set\n",
    "print(\"\\nClassification Report on validation set:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Feature importance (sorted)\n",
    "importance = model.get_booster().get_score(importance_type='weight')\n",
    "sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "for feature, score in sorted_importance[:10]:  # Display top 10 features\n",
    "    print(f\"{feature}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "\n",
    "# # Define the model parameters\n",
    "# # params = {\n",
    "# #     'learning_rate': 0.01,\n",
    "# #     'max_depth': 6,\n",
    "# #     'min_child_weight': 3,\n",
    "# #     'reg_lambda': 5.0,\n",
    "# #     'reg_alpha': 10,\n",
    "# #     'n_estimators': 1000,\n",
    "# #     'subsample': 0.7,\n",
    "# #     'colsample_bytree': 0.7,\n",
    "# #     'objective': 'reg:squarederror',\n",
    "# #     'eval_metric' : ['rmse', 'mae'],\n",
    "# #     'tree_method': 'hist',\n",
    "# #     'device': 'cuda',\n",
    "# #     'random_state': 1  # Set the seed here\n",
    "\n",
    "# # }\n",
    "\n",
    "# #params =  {'learning_rate': 0.020534279384947673, 'max_depth': 9, 'n_estimators': 1800, 'subsample': 0.6817057463318822, 'colsample_bytree': 0.7324724625794548, 'reg_lambda': 0.2145407717059405, 'reg_alpha': 0.7071638502266191, 'min_child_weight': 1}\n",
    "# params = {'learning_rate': 0.02309808922520973, 'max_depth': 7, 'n_estimators': 1500, 'subsample': 0.6895773994724415, 'colsample_bytree': 0.781315211059423, 'reg_lambda': 3.726939197847549, 'reg_alpha': 1.7408154972521837, 'min_child_weight': 40}\n",
    "\n",
    "# # Initialize the XGBRegressor with the specified parameters\n",
    "# model = xgb.XGBRegressor(**params)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions on validation set\n",
    "# y_pred = np.clip(np.round(model.predict(X_val)), 0, 5)\n",
    "\n",
    "# # Calculate accuracy and RMSE on validation set\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(f\"Accuracy on validation set: {accuracy:.4f}\")\n",
    "\n",
    "# rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "# print(f\"Root Mean Squared Error on validation set: {rmse:.4f}\")\n",
    "\n",
    "# # Predictions on training set\n",
    "# y_pred_train = np.clip(np.round(model.predict(X_train)), 0, 5)\n",
    "\n",
    "# # Calculate accuracy and RMSE on training set\n",
    "# train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "# print(f\"Accuracy on training set: {train_accuracy:.4f}\")\n",
    "\n",
    "# train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "# print(f\"Root Mean Squared Error on training set: {train_rmse:.4f}\")\n",
    "\n",
    "# # Classification report on validation set\n",
    "# print(\"\\nClassification Report on validation set:\")\n",
    "# print(classification_report(y_val, y_pred))\n",
    "\n",
    "# # Feature importance (sorted)\n",
    "# importance = model.get_booster().get_score(importance_type='weight')\n",
    "# sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "# print(\"\\nTop 10 Important Features:\")\n",
    "# for feature, score in sorted_importance[:10]:\n",
    "#     print(f\"{feature}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-25 06:47:16,366] A new study created in memory with name: no-name-cfb2b2c3-a881-4cfe-90e4-4e07a8debdc5\n",
      "[I 2024-11-25 06:47:22,671] Trial 0 finished with value: 0.8641847525987738 and parameters: {'learning_rate': 0.019781168098986078, 'subsample': 0.6429787232281367, 'colsample_bytree': 0.6682042794393254, 'reg_lambda': 0.3295351616681135, 'reg_alpha': 0.40874051149759405, 'min_child_weight': 3}. Best is trial 0 with value: 0.8641847525987738.\n",
      "[I 2024-11-25 06:47:28,451] Trial 1 finished with value: 0.7733622514543377 and parameters: {'learning_rate': 0.07001375350812115, 'subsample': 0.7659083894468397, 'colsample_bytree': 0.6144883875377759, 'reg_lambda': 0.6483108075975549, 'reg_alpha': 0.2025133072645006, 'min_child_weight': 2}. Best is trial 1 with value: 0.7733622514543377.\n",
      "[I 2024-11-25 06:47:35,369] Trial 2 finished with value: 0.9694374230436187 and parameters: {'learning_rate': 0.013248552007252107, 'subsample': 0.6109030783180616, 'colsample_bytree': 0.662560874105006, 'reg_lambda': 0.06506027065535314, 'reg_alpha': 0.15267161696000509, 'min_child_weight': 3}. Best is trial 1 with value: 0.7733622514543377.\n",
      "[I 2024-11-25 06:47:42,199] Trial 3 finished with value: 0.8006764655887987 and parameters: {'learning_rate': 0.03621019015655461, 'subsample': 0.6505190340828023, 'colsample_bytree': 0.7651493810643614, 'reg_lambda': 0.3257921636279072, 'reg_alpha': 0.12373951027545865, 'min_child_weight': 2}. Best is trial 1 with value: 0.7733622514543377.\n",
      "[I 2024-11-25 06:47:48,987] Trial 4 finished with value: 0.9555403313522483 and parameters: {'learning_rate': 0.015366174419592104, 'subsample': 0.609512581877047, 'colsample_bytree': 0.7678155347036035, 'reg_lambda': 0.6845056019375448, 'reg_alpha': 0.0850016908813454, 'min_child_weight': 2}. Best is trial 1 with value: 0.7733622514543377.\n",
      "[I 2024-11-25 06:47:55,462] Trial 5 finished with value: 0.9969699316358479 and parameters: {'learning_rate': 0.01125543713259486, 'subsample': 0.6417714383401858, 'colsample_bytree': 0.6271955837965334, 'reg_lambda': 0.5295091983595084, 'reg_alpha': 0.520872079029114, 'min_child_weight': 2}. Best is trial 1 with value: 0.7733622514543377.\n",
      "[I 2024-11-25 06:48:01,706] Trial 6 finished with value: 0.7723320628373681 and parameters: {'learning_rate': 0.08503759764465421, 'subsample': 0.7602554199102722, 'colsample_bytree': 0.6160708072950953, 'reg_lambda': 0.294072740424843, 'reg_alpha': 0.16051878753304855, 'min_child_weight': 3}. Best is trial 6 with value: 0.7723320628373681.\n",
      "[I 2024-11-25 06:48:07,990] Trial 7 finished with value: 0.9971296384741388 and parameters: {'learning_rate': 0.010826057212642253, 'subsample': 0.7071281222232506, 'colsample_bytree': 0.6619866497305166, 'reg_lambda': 0.17875878370675558, 'reg_alpha': 0.6170568915148648, 'min_child_weight': 1}. Best is trial 6 with value: 0.7723320628373681.\n",
      "[I 2024-11-25 06:48:15,037] Trial 8 finished with value: 0.8328448674563634 and parameters: {'learning_rate': 0.02339549674666325, 'subsample': 0.6835523908958345, 'colsample_bytree': 0.7453888355701359, 'reg_lambda': 0.2267588355338639, 'reg_alpha': 0.2999866362441691, 'min_child_weight': 2}. Best is trial 6 with value: 0.7723320628373681.\n",
      "[I 2024-11-25 06:48:21,202] Trial 9 finished with value: 0.7992831182908302 and parameters: {'learning_rate': 0.037377978424104216, 'subsample': 0.7397707931370322, 'colsample_bytree': 0.6673854750184199, 'reg_lambda': 0.3203396706040551, 'reg_alpha': 0.13536013276943706, 'min_child_weight': 1}. Best is trial 6 with value: 0.7723320628373681.\n",
      "[I 2024-11-25 06:48:27,636] Trial 10 finished with value: 0.7661218409360457 and parameters: {'learning_rate': 0.0772813484505639, 'subsample': 0.7927983308903827, 'colsample_bytree': 0.7165107575699793, 'reg_lambda': 0.9614748813434985, 'reg_alpha': 0.7868918610729997, 'min_child_weight': 3}. Best is trial 10 with value: 0.7661218409360457.\n",
      "[I 2024-11-25 06:48:33,924] Trial 11 finished with value: 0.7588123472532251 and parameters: {'learning_rate': 0.08748281247944359, 'subsample': 0.7995986950841062, 'colsample_bytree': 0.7148792079582755, 'reg_lambda': 0.9949385383449679, 'reg_alpha': 0.7615649809168463, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:48:40,112] Trial 12 finished with value: 0.7657060352929683 and parameters: {'learning_rate': 0.060807980947741085, 'subsample': 0.7961615294423805, 'colsample_bytree': 0.7204795093693752, 'reg_lambda': 0.9979887835832233, 'reg_alpha': 0.7787452928437258, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:48:46,260] Trial 13 finished with value: 0.7809434111796886 and parameters: {'learning_rate': 0.05360301533474141, 'subsample': 0.796760658865041, 'colsample_bytree': 0.7153655685343813, 'reg_lambda': 0.967782993643334, 'reg_alpha': 0.7760234342258253, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:48:52,833] Trial 14 finished with value: 0.7819622549412895 and parameters: {'learning_rate': 0.05197782575526919, 'subsample': 0.7278993587562423, 'colsample_bytree': 0.7964302158176355, 'reg_lambda': 0.845284970489622, 'reg_alpha': 0.648451644766255, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:48:58,840] Trial 15 finished with value: 0.7654980477744517 and parameters: {'learning_rate': 0.058537850814507714, 'subsample': 0.7746165146233478, 'colsample_bytree': 0.694282033649058, 'reg_lambda': 0.7986655589983086, 'reg_alpha': 0.6694773382692354, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:49:05,059] Trial 16 finished with value: 0.783183117909435 and parameters: {'learning_rate': 0.043738280584388445, 'subsample': 0.7695921258739665, 'colsample_bytree': 0.6934114279960032, 'reg_lambda': 0.8091062990633939, 'reg_alpha': 0.6517032457074976, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:49:11,752] Trial 17 finished with value: 0.8030594365292809 and parameters: {'learning_rate': 0.03057364827973495, 'subsample': 0.7341459410599073, 'colsample_bytree': 0.6927887944686844, 'reg_lambda': 0.7963708861200242, 'reg_alpha': 0.5091688632510945, 'min_child_weight': 2}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:49:18,106] Trial 18 finished with value: 0.7673679060168684 and parameters: {'learning_rate': 0.06463312788742331, 'subsample': 0.775988633503418, 'colsample_bytree': 0.737407173102407, 'reg_lambda': 0.6631967771204308, 'reg_alpha': 0.6858746212319407, 'min_child_weight': 1}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:49:24,075] Trial 19 finished with value: 0.7719196024272638 and parameters: {'learning_rate': 0.08825657914191885, 'subsample': 0.7082334761744502, 'colsample_bytree': 0.6842697553753753, 'reg_lambda': 0.5046625715072136, 'reg_alpha': 0.5357550736692391, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:49:30,502] Trial 20 finished with value: 0.7692332190063882 and parameters: {'learning_rate': 0.046937468416550644, 'subsample': 0.7549148765954049, 'colsample_bytree': 0.6458496327565231, 'reg_lambda': 0.8699620662666703, 'reg_alpha': 0.014196821804182691, 'min_child_weight': 2}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:49:36,713] Trial 21 finished with value: 0.7675753868658719 and parameters: {'learning_rate': 0.06255017685358939, 'subsample': 0.7971791568884417, 'colsample_bytree': 0.7177371304315596, 'reg_lambda': 0.9486966134731145, 'reg_alpha': 0.7275679201520552, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:49:43,290] Trial 22 finished with value: 0.7675753868658719 and parameters: {'learning_rate': 0.06112665324546051, 'subsample': 0.7806647198447312, 'colsample_bytree': 0.7414041922415673, 'reg_lambda': 0.9901564270606926, 'reg_alpha': 0.5863710401177844, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:49:49,606] Trial 23 finished with value: 0.7760343416132158 and parameters: {'learning_rate': 0.07428935246381636, 'subsample': 0.7490278801811331, 'colsample_bytree': 0.7080609871923688, 'reg_lambda': 0.7429417615838956, 'reg_alpha': 0.7134229618412398, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:49:55,714] Trial 24 finished with value: 0.7745966692414834 and parameters: {'learning_rate': 0.05245895177106066, 'subsample': 0.783136603957954, 'colsample_bytree': 0.6826627322742564, 'reg_lambda': 0.8879930325787275, 'reg_alpha': 0.437681407989901, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:50:01,955] Trial 25 finished with value: 0.7852136712730746 and parameters: {'learning_rate': 0.038284417960981434, 'subsample': 0.7802978600118335, 'colsample_bytree': 0.7274909722790048, 'reg_lambda': 0.899928768512117, 'reg_alpha': 0.7947194183490377, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:50:08,721] Trial 26 finished with value: 0.8144788494358487 and parameters: {'learning_rate': 0.02641642719159156, 'subsample': 0.7993266103807245, 'colsample_bytree': 0.7589899202813132, 'reg_lambda': 0.5863004255738733, 'reg_alpha': 0.31421151649477036, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:50:14,924] Trial 27 finished with value: 0.7704742521644754 and parameters: {'learning_rate': 0.0890830959095371, 'subsample': 0.7471775952430442, 'colsample_bytree': 0.7021686861295545, 'reg_lambda': 0.7500403710974802, 'reg_alpha': 0.7253920386567979, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:50:21,811] Trial 28 finished with value: 0.7776741413109313 and parameters: {'learning_rate': 0.05968833283858613, 'subsample': 0.677611631398423, 'colsample_bytree': 0.6470970150930746, 'reg_lambda': 0.4345511723240421, 'reg_alpha': 0.5712414553423947, 'min_child_weight': 2}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:50:28,291] Trial 29 finished with value: 0.7854164378797529 and parameters: {'learning_rate': 0.04257196328495745, 'subsample': 0.725535059382531, 'colsample_bytree': 0.7259150559291703, 'reg_lambda': 0.9123469841212318, 'reg_alpha': 0.433005863336118, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:50:34,723] Trial 30 finished with value: 0.880612120633352 and parameters: {'learning_rate': 0.01885373070110569, 'subsample': 0.7846807484000624, 'colsample_bytree': 0.7858061520776928, 'reg_lambda': 0.9898457443744496, 'reg_alpha': 0.7492824496186832, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:50:40,658] Trial 31 finished with value: 0.7648737458782057 and parameters: {'learning_rate': 0.07562667925144609, 'subsample': 0.7905483045340296, 'colsample_bytree': 0.6809213065081238, 'reg_lambda': 0.9249388651084163, 'reg_alpha': 0.7801938920937059, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:50:46,910] Trial 32 finished with value: 0.7710940197285585 and parameters: {'learning_rate': 0.07424528503465581, 'subsample': 0.768827407475011, 'colsample_bytree': 0.6839456602098175, 'reg_lambda': 0.8180281126576636, 'reg_alpha': 0.67810255996141, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:50:52,861] Trial 33 finished with value: 0.7704742521644754 and parameters: {'learning_rate': 0.06950816616562819, 'subsample': 0.7625942056026739, 'colsample_bytree': 0.6735829426053817, 'reg_lambda': 0.740128815179623, 'reg_alpha': 0.7939773233707086, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:50:59,074] Trial 34 finished with value: 0.7735681245450216 and parameters: {'learning_rate': 0.07938343223234341, 'subsample': 0.78742228563505, 'colsample_bytree': 0.6521665240531087, 'reg_lambda': 0.925824387582229, 'reg_alpha': 0.7367135015007092, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:51:05,581] Trial 35 finished with value: 0.7667451266046459 and parameters: {'learning_rate': 0.05566664836379434, 'subsample': 0.7719052186438742, 'colsample_bytree': 0.7016498258483999, 'reg_lambda': 0.4189792021070621, 'reg_alpha': 0.6214873732400247, 'min_child_weight': 2}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:51:11,655] Trial 36 finished with value: 0.7741854153397885 and parameters: {'learning_rate': 0.0673499747044365, 'subsample': 0.7982941857474637, 'colsample_bytree': 0.7289197709244821, 'reg_lambda': 0.8619356467681512, 'reg_alpha': 0.69160709364579, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:51:18,155] Trial 37 finished with value: 0.7958891194418101 and parameters: {'learning_rate': 0.03220213148542224, 'subsample': 0.7591644962833913, 'colsample_bytree': 0.7534264231536788, 'reg_lambda': 0.5872579708163167, 'reg_alpha': 0.7476249174114342, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:51:24,835] Trial 38 finished with value: 0.7642489340005775 and parameters: {'learning_rate': 0.07925354641778247, 'subsample': 0.7869131671707492, 'colsample_bytree': 0.6280905104685255, 'reg_lambda': 0.773662843792334, 'reg_alpha': 0.6586746766832895, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:51:30,654] Trial 39 finished with value: 0.7692332190063882 and parameters: {'learning_rate': 0.07833502895407332, 'subsample': 0.6648146588445732, 'colsample_bytree': 0.6299598031590172, 'reg_lambda': 0.7089501335014271, 'reg_alpha': 0.5744111143787833, 'min_child_weight': 2}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:51:36,667] Trial 40 finished with value: 0.7677828116465442 and parameters: {'learning_rate': 0.08338363961132585, 'subsample': 0.626568129891997, 'colsample_bytree': 0.6298044592339782, 'reg_lambda': 0.5854592266177716, 'reg_alpha': 0.2598031983618772, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:51:42,824] Trial 41 finished with value: 0.7692332190063882 and parameters: {'learning_rate': 0.06998347441958884, 'subsample': 0.7883685609347204, 'colsample_bytree': 0.6136212376746473, 'reg_lambda': 0.790865299058909, 'reg_alpha': 0.6527755690339965, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:51:48,848] Trial 42 finished with value: 0.7673679060168684 and parameters: {'learning_rate': 0.057742442968386744, 'subsample': 0.7730108936899113, 'colsample_bytree': 0.6021835217370796, 'reg_lambda': 0.9251526187800904, 'reg_alpha': 0.7610585546114469, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:51:54,892] Trial 43 finished with value: 0.7780835512583809 and parameters: {'learning_rate': 0.04740060986853064, 'subsample': 0.7882112362207023, 'colsample_bytree': 0.658230222390336, 'reg_lambda': 0.8579708151956532, 'reg_alpha': 0.7041025090549794, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:52:01,434] Trial 44 finished with value: 0.7729503407718784 and parameters: {'learning_rate': 0.06997023116992623, 'subsample': 0.7632040021599137, 'colsample_bytree': 0.6723443386540885, 'reg_lambda': 0.9557370811683928, 'reg_alpha': 0.7993292048708731, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:52:07,566] Trial 45 finished with value: 0.7669527759318393 and parameters: {'learning_rate': 0.08117910551315287, 'subsample': 0.7996691056488807, 'colsample_bytree': 0.7111755671429344, 'reg_lambda': 0.05434744325473406, 'reg_alpha': 0.48551951323901194, 'min_child_weight': 2}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:52:14,311] Trial 46 finished with value: 0.9782670235813694 and parameters: {'learning_rate': 0.01315339047195717, 'subsample': 0.776627376696414, 'colsample_bytree': 0.700164689543521, 'reg_lambda': 0.9866863489850276, 'reg_alpha': 0.6343297791944991, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:52:21,113] Trial 47 finished with value: 0.778492745896847 and parameters: {'learning_rate': 0.050073598694146584, 'subsample': 0.7905578648846895, 'colsample_bytree': 0.6911478448587054, 'reg_lambda': 0.7771424448322541, 'reg_alpha': 0.6630135558889584, 'min_child_weight': 1}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:52:27,156] Trial 48 finished with value: 0.7708874859041148 and parameters: {'learning_rate': 0.08848806585013742, 'subsample': 0.7518800487887974, 'colsample_bytree': 0.7210480382217543, 'reg_lambda': 0.13515695305445508, 'reg_alpha': 0.5952463153432456, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:52:33,486] Trial 49 finished with value: 0.7675753868658719 and parameters: {'learning_rate': 0.06493743951854439, 'subsample': 0.7203739524961624, 'colsample_bytree': 0.7347382595385715, 'reg_lambda': 0.8312512047738642, 'reg_alpha': 0.7684246912801583, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:52:40,363] Trial 50 finished with value: 0.7934846150965401 and parameters: {'learning_rate': 0.040858200644144145, 'subsample': 0.7422156238373355, 'colsample_bytree': 0.639998721963784, 'reg_lambda': 0.6401129639117646, 'reg_alpha': 0.7033661408310103, 'min_child_weight': 2}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:52:46,646] Trial 51 finished with value: 0.7686119509936019 and parameters: {'learning_rate': 0.07596787814324654, 'subsample': 0.79003280468638, 'colsample_bytree': 0.746525409473427, 'reg_lambda': 0.956295001590622, 'reg_alpha': 0.7557523801711621, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:52:53,127] Trial 52 finished with value: 0.7745966692414834 and parameters: {'learning_rate': 0.07395247515593706, 'subsample': 0.7914738584828884, 'colsample_bytree': 0.709035296840135, 'reg_lambda': 0.9998323696573407, 'reg_alpha': 0.7746844837052466, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:52:59,685] Trial 53 finished with value: 0.7729503407718784 and parameters: {'learning_rate': 0.06473430896985946, 'subsample': 0.7763989069740102, 'colsample_bytree': 0.7207653397531731, 'reg_lambda': 0.8949666590420262, 'reg_alpha': 0.7174733494039319, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:53:05,955] Trial 54 finished with value: 0.7797190413472602 and parameters: {'learning_rate': 0.08165712654172373, 'subsample': 0.6032333527396947, 'colsample_bytree': 0.6927937406254246, 'reg_lambda': 0.928026477640813, 'reg_alpha': 0.6650132121662956, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:53:12,170] Trial 55 finished with value: 0.770680896730928 and parameters: {'learning_rate': 0.057040059296635505, 'subsample': 0.7810995945295083, 'colsample_bytree': 0.7159760909333885, 'reg_lambda': 0.8476137001187989, 'reg_alpha': 0.7958766669495488, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:53:18,529] Trial 56 finished with value: 0.7667451266046459 and parameters: {'learning_rate': 0.08991333401199576, 'subsample': 0.7929566625090245, 'colsample_bytree': 0.6782577168656456, 'reg_lambda': 0.9572209843748493, 'reg_alpha': 0.5424589000382976, 'min_child_weight': 3}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[I 2024-11-25 06:53:25,105] Trial 57 finished with value: 0.7719196024272638 and parameters: {'learning_rate': 0.06196702586405375, 'subsample': 0.7692777809344354, 'colsample_bytree': 0.7695078736025728, 'reg_lambda': 0.8995199000724943, 'reg_alpha': 0.6099741731590868, 'min_child_weight': 1}. Best is trial 11 with value: 0.7588123472532251.\n",
      "[W 2024-11-25 06:53:28,895] Trial 58 failed with parameters: {'learning_rate': 0.07282456410269945, 'subsample': 0.7816897774216693, 'colsample_bytree': 0.7323991834311949, 'reg_lambda': 0.873317587048941, 'reg_alpha': 0.7345887583976186, 'min_child_weight': 3} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_50575/1762543333.py\", line 59, in objective\n",
      "    model.fit(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1108, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 2101, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2024-11-25 06:53:28,896] Trial 58 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Optimize the objective\u001b[39;00m\n\u001b[1;32m     73\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Number of trials\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[27], line 59\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     56\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_split\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Stops if no improvement for 50 rounds\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     66\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(np\u001b[38;5;241m.\u001b[39mround(model\u001b[38;5;241m.\u001b[39mpredict(X_val_split)), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1108\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    # param = {\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'max_depth': 9,\n",
    "    #     'n_estimators': 1000,  # Use early stopping to limit unnecessary rounds\n",
    "    #     'subsample': 0.8,\n",
    "    #     'colsample_bytree': 0.8,\n",
    "    #     'early_stopping_rounds': 500,\n",
    "    #     'objective': 'reg:squarederror',\n",
    "    #     'eval_metric': 'rmse',\n",
    "    #     'tree_method': 'hist',\n",
    "    #     'device': 'cuda',\n",
    "    #     'random_state': 1,\n",
    "        \n",
    "    #     # Hyperparameters to optimize\n",
    "    #     'learning_rate': trial.suggest_float('learning_rate', 1e-3, 2e-1, log=True),\n",
    "    #     'max_depth': trial.suggest_int('max_depth', 8, 10),  # Choose a reasonable range\n",
    "    #     'n_estimators': trial.suggest_int('n_estimators', 1200, 2000, step=100),\n",
    "    #     'subsample': trial.suggest_float('subsample', 0.6, 0.8),\n",
    "    #     'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.8),\n",
    "    #     'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 1.0, log=True),\n",
    "    #     'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0, log=True),\n",
    "    #     'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),\n",
    "    # }\n",
    "\n",
    "    param = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda',\n",
    "        'random_state': 1,\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 9,\n",
    "        \n",
    "        # Hyperparameters to optimize\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.09, log=True),\n",
    "        #'max_depth': trial.suggest_int('max_depth', 8, 9),  # Near 9\n",
    "      #  'n_estimators': trial.suggest_int('n_estimators', 50, 100, step=100),  # Around 1800\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),  # Around 0.68\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.8),  # Around 0.73\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1),  # Near 0.21\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.8),  # Around 0.71\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 3),  # Slightly broader\n",
    "    }\n",
    "    \n",
    "    # Split the data into train/validation sets\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X_clean, y, test_size=0.20, random_state=1\n",
    "    )\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = xgb.XGBRegressor(**param)\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    model.fit(\n",
    "        X_train_split, y_train_split,\n",
    "        eval_set=[(X_val_split, y_val_split)],  # Stops if no improvement for 50 rounds\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = np.clip(np.round(model.predict(X_val_split)), 0, 5)\n",
    "    \n",
    "    # Calculate RMSE as the optimization target\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_split, y_pred))\n",
    "    return rmse  # Optuna minimizes the objective\n",
    "\n",
    "# Optimize the objective\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)  # Number of trials\n",
    "print(\"Best Parameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.08748281247944359, 'subsample': 0.7995986950841062, 'colsample_bytree': 0.7148792079582755, 'reg_lambda': 0.9949385383449679, 'reg_alpha': 0.7615649809168463, 'min_child_weight': 3}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "# import numpy as np\n",
    "\n",
    "# # Define parameter dictionary for XGBClassifier\n",
    "# params = {\n",
    "#     'objective': 'multi:softmax',\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 9,\n",
    "#     'min_child_weight': 3,\n",
    "#     'reg_lambda': 1.0,\n",
    "#     'reg_alpha': 0.1,\n",
    "#     'n_estimators': 1000,\n",
    "#     'subsample': 0.8,\n",
    "#     'colsample_bytree': 0.8,\n",
    "#     'num_class': len(np.unique(y_train)),\n",
    "#     'objective': 'reg:squarederror',  # For regression\n",
    "#     'eval_metric': 'rmse'            # RMSE as evaluation metric\n",
    "# }\n",
    "\n",
    "# # Initialize and train XGBClassifier model\n",
    "# model = XGBClassifier(**params, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions on validation set\n",
    "# y_pred = model.predict(X_val)\n",
    "\n",
    "# # Calculate accuracy and RMSE on validation set\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(\"Accuracy on validation set:\", accuracy)\n",
    "\n",
    "# rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "# print(\"Root Mean Squared Error on validation set:\", rmse)\n",
    "\n",
    "# # Predictions on training set\n",
    "# y_pred_train = model.predict(X_train)\n",
    "\n",
    "# # Calculate accuracy and RMSE on training set\n",
    "# train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "# print(f\"Accuracy on training set: {train_accuracy:.4f}\")\n",
    "\n",
    "# train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "# print(\"Root Mean Squared Error on training set:\", train_rmse)\n",
    "\n",
    "# # Classification report on validation set\n",
    "# print(\"\\nClassification Report on validation set:\")\n",
    "# print(classification_report(y_val, y_pred))\n",
    "\n",
    "# # Feature importance (sorted)\n",
    "# importance = model.get_booster().get_score(importance_type='weight')\n",
    "# sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "# print(\"\\nTop 10 Important Features:\")\n",
    "# for feature, score in sorted_importance[:10]:  # Display top 10 features\n",
    "#     print(f\"{feature}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test: 0.9987\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy test: {accuracy:.4f}\")\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_clean = X_test_clean[X_clean.columns]  # Align columns with the training set\n",
    "y_pred_test = np.clip(np.round(model.predict(X_test_clean)), 0, 5)\n",
    "\n",
    "# for id,y_pred in zip(ID_test,y_pred_test):\n",
    "#     print(str(id)+\",\"+str(int(y_pred)))\n",
    "\n",
    "df_output = pd.DataFrame({'id': ID_test, 'price': y_pred_test})\n",
    "\n",
    "# Convert y_pred to integers\n",
    "df_output['price'] = df_output['price'].astype(int)\n",
    "\n",
    "# Specify the file path\n",
    "file_path = 'data/my_preds_with_reviews_name_and_des_redone.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df_output.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter dictionary for XGBClassifier\n",
    "# params = {\n",
    "#     'learning_rate': 0.1,\n",
    "#     'n_estimators': 150,  # Equivalent to the number of boosting rounds\n",
    "#     'max_depth': 9,\n",
    "#     'reg_lambda': 0.1,     # L2 regularization parameter\n",
    "# }\n",
    "\n",
    "# # Initialize the model with the parameters\n",
    "# model = XGBClassifier(**params, random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# y_pred = model.predict(X_val)\n",
    "\n",
    "# # Evaluate the model using accuracy\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(\"Accuracy on validation set:\", accuracy)\n",
    "\n",
    "# # Calculate RMSE between predicted and true quantile classes\n",
    "# rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "# print(\"Root Mean Squared Error on validation set:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import xgboost as xgb\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the base model\n",
    "# model = xgb.XGBRegressor(\n",
    "#     reg_lambda=1.0,\n",
    "#     reg_alpha=0.1,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     objective='reg:squarederror',\n",
    "#     eval_metric='rmse',\n",
    "#     random_state=1\n",
    "# )\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.001, 0.01, 0.1],\n",
    "#     'max_depth': [3, 6, 9],\n",
    "#     'min_child_weight': [1, 3, 5],\n",
    "#     'n_estimators': [100, 500, 1000]\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=model,\n",
    "#     param_grid=param_grid,\n",
    "#     cv=5,  # 5-fold cross-validation\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     n_jobs=-1,  # Use all available cores\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# # Fit the grid search to the data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and best score\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = np.sqrt(-grid_search.best_score_)  # Convert MSE to RMSE\n",
    "\n",
    "# print(\"Best parameters:\", best_params)\n",
    "# print(\"Best RMSE:\", best_score)\n",
    "\n",
    "# # Use the best model for predictions\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = np.clip(np.round(best_model.predict(X_val)), 0, 5)\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import xgboost as xgb\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the base model\n",
    "# model = xgb.XGBRegressor(\n",
    "#     reg_lambda=1.0,\n",
    "#     reg_alpha=0.1,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     objective='reg:squarederror',\n",
    "#     eval_metric='rmse',\n",
    "#     random_state=1\n",
    "# )\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.001, 0.01, 0.1],\n",
    "#     'max_depth': [3, 6, 9],\n",
    "#     'min_child_weight': [1, 3, 5],\n",
    "#     'n_estimators': [100, 500, 1000]\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=model,\n",
    "#     param_grid=param_grid,\n",
    "#     cv=5,  # 5-fold cross-validation\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     n_jobs=-1,  # Use all available cores\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# # Fit the grid search to the data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and best score\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = np.sqrt(-grid_search.best_score_)  # Convert MSE to RMSE\n",
    "\n",
    "# print(\"Best parameters:\", best_params)\n",
    "# print(\"Best RMSE:\", best_score)\n",
    "\n",
    "# # Use the best model for predictions\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = np.clip(np.round(best_model.predict(X_val)), 0, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First XGBoost model\n",
    "# params1 = {\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 9,\n",
    "#     'min_child_weight': 3,\n",
    "#     'reg_lambda': 1.0,\n",
    "#     'reg_alpha': 0.1,\n",
    "#     'n_estimators': 1000,\n",
    "#     'subsample': 0.8,\n",
    "#     'colsample_bytree': 0.8,\n",
    "#     'objective': 'reg:squarederror',\n",
    "#     'eval_metric': ['rmse', 'mae'],\n",
    "#     'tree_method': 'hist',\n",
    "#     'device': 'cuda',\n",
    "#     'random_state': 1\n",
    "# }\n",
    "\n",
    "# model1 = xgb.XGBRegressor(**params1)\n",
    "\n",
    "# # Train the first model\n",
    "# model1.fit(X_train, y_train)\n",
    "\n",
    "# # Get predictions from the first model\n",
    "# X_train_pred = model1.predict(X_train).reshape(-1, 1)\n",
    "# X_val_pred = model1.predict(X_val).reshape(-1, 1)\n",
    "\n",
    "# # Ensure amenities_df is a list of column names\n",
    "# selected_columns = list(amenities_df.columns)  # Replace with actual column names if needed\n",
    "# X_train2 = X_train[selected_columns]    \n",
    "# X_val2 = X_val[selected_columns]    \n",
    "# # Combine the predictions with extra training data\n",
    "# X_train_combined = np.hstack((X_train2, X_train_pred))\n",
    "# X_val_combined = np.hstack((X_val2, X_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Second XGBoost model\n",
    "# params2 = {\n",
    "#     'learning_rate': 0.1,\n",
    "#     'max_depth': 5,\n",
    "#     'min_child_weight': 75,\n",
    "#     'reg_lambda': 0.8,\n",
    "#     'reg_alpha': 0.2,\n",
    "#     'n_estimators': 800,\n",
    "#     'subsample': 0.9,\n",
    "#     'colsample_bytree': 0.9,\n",
    "#     'objective': 'reg:squarederror',\n",
    "#     'eval_metric': ['rmse', 'mae'],\n",
    "#     'random_state': 7\n",
    "# }\n",
    "\n",
    "# # Create the second XGBoost model without scaling\n",
    "# model2 = xgb.XGBRegressor(**params2)\n",
    "\n",
    "# # Train the second model\n",
    "# model2.fit(X_train_combined, y_train)\n",
    "\n",
    "# # Make predictions using the second model\n",
    "# y_pred = np.clip(np.round(model2.predict(X_val_combined)), 0, 5)\n",
    "\n",
    "# # Calculate accuracy and RMSE on validation set\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(f\"Accuracy on validation set: {accuracy:.4f}\")\n",
    "\n",
    "# rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "# print(f\"Root Mean Squared Error on validation set: {rmse:.4f}\")\n",
    "\n",
    "# # Predictions on training set\n",
    "# y_pred_train = np.clip(np.round(model2.predict(X_train_combined)), 0, 5)\n",
    "\n",
    "# # Calculate accuracy and RMSE on training set\n",
    "# train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "# print(f\"Accuracy on training set: {train_accuracy:.4f}\")\n",
    "\n",
    "# train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "# print(f\"Root Mean Squared Error on training set: {train_rmse:.4f}\")\n",
    "\n",
    "# # Classification report on validation set\n",
    "# print(\"\\nClassification Report on validation set:\")\n",
    "# print(classification_report(y_val, y_pred))\n",
    "\n",
    "# # Feature importance (sorted) for the second model\n",
    "# importance = model2.get_booster().get_score(importance_type='weight')\n",
    "# sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "# print(\"\\nTop 10 Important Features:\")\n",
    "# for feature, score in sorted_importance[:10]:\n",
    "#     print(f\"{feature}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get predictions from the first model\n",
    "# X_test_pred = model1.predict(X_test_clean).reshape(-1, 1)\n",
    "# # Ensure amenities_df is a list of column names\n",
    "# selected_columns = list(amenities_df.columns)  # Replace with actual column names if needed\n",
    "# X_test2 = X_test_clean[selected_columns]    \n",
    "# # Combine the predictions with extra training data\n",
    "# X_train_combined = np.hstack((X_test2, X_test_pred))\n",
    "# y_pred_test = np.clip(np.round(model2.predict(X_train_combined)), 0, 5)\n",
    "\n",
    "# # for id,y_pred in zip(ID_test,y_pred_test):\n",
    "# #     print(str(id)+\",\"+str(int(y_pred)))\n",
    "\n",
    "# df_output = pd.DataFrame({'id': ID_test, 'price': y_pred_test})\n",
    "\n",
    "# # Convert y_pred to integers\n",
    "# df_output['price'] = df_output['price'].astype(int)\n",
    "\n",
    "# # Specify the file path\n",
    "# file_path = 'data/my_preds2.csv'\n",
    "\n",
    "# # Write the DataFrame to a CSV file\n",
    "# df_output.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
