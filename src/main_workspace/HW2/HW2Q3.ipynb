{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties of Variance (I am not going to prove these):\n",
    "\n",
    "$Var(cX) = c^2 * Var(X)$ \n",
    "\n",
    "\n",
    "$Var(X_i + X_j) = Var(X_i) + Var(X_j) + 2Cov(X_i,X_j)$\n",
    "\n",
    "Problem Statement Implies: \n",
    "\n",
    "$\\rho*\\sigma^2 = Cov(X_i,X_j)$\n",
    "\n",
    "$Var(X_i) = \\sigma^2$\n",
    "\n",
    "\\begin{align*}\n",
    "Var\\left(\\frac{1}{M}\\sum_{i=1}^M X_i\\right) &= \\frac{1}{M^2} Var\\left(\\sum_{i=1}^M X_i\\right)\\\\\n",
    "&= \\frac{1}{M^2} Var(\\sum_{i=1}^M X_i)\\\\\n",
    "\n",
    "&=\\frac{1}{M^2}\\left[\\sum_{i=1}^MVar(X_i)+\\sum_{i=1}^M\\sum_{j=1, j\\neq i}^MCov(X_i,X_j)\\right]\\\\\n",
    "\n",
    "&=\\frac{1}{M^2}\\left[M\\sigma^2+M(M-1)\\rho\\sigma^2\\right] \\\\\n",
    "&=\\frac{1}{M^2}\\left[M\\sigma^2+M^2\\rho\\sigma^2-M\\rho\\sigma^2\\right]\\\\\n",
    "&=\\frac{1}{M^2}\\left[M\\sigma^2(1-\\rho)+M^2\\rho\\sigma^2\\right] \\\\\n",
    "&=\\frac{M\\sigma^2(1-\\rho)}{M^2}+\\rho\\sigma^2 \\\\\n",
    "&=\\rho\\sigma^2 + \\frac{1-\\rho}{M}\\sigma^2\\\\\n",
    "\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3B i to ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Function to create a bootstrapped dataset given features and labels\n",
    "def bootstrap_dataset(X, y, N):\n",
    "    \"\"\"\n",
    "    Function to create a bootstrapped training dataset D âˆ¼ D by uniformly sampling N samples with replacement.\n",
    "    \n",
    "    Args:\n",
    "    - X: Features of the dataset (pandas DataFrame).\n",
    "    - y: Labels of the dataset (pandas Series).\n",
    "    - N: The number of samples to draw.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple (X_bootstrap, y_bootstrap) where both are bootstrapped versions of X and y.\n",
    "    \"\"\"\n",
    "    bootstrap_indices = np.random.choice(X.index, size=N, replace=True)\n",
    "    X_bootstrap = X.loc[bootstrap_indices]\n",
    "    y_bootstrap = y.loc[bootstrap_indices]\n",
    "    return X_bootstrap, y_bootstrap\n",
    "\n",
    "# Load the training and test datasets\n",
    "X_train = pd.read_csv('wine_true_D.csv',header=None)  # Training features\n",
    "y_train = pd.read_csv('wine_true_y.csv', header=None).squeeze()  # Training labels\n",
    "X_test = pd.read_csv('wine_test.csv')  # Test data just\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "\n",
    "# Bootstrapped dataset size and number of datasets\n",
    "N = 1024\n",
    "num_bootstrap_datasets = 3000\n",
    "\n",
    "# Initialize to store predictions for each tree on each bootstrap dataset\n",
    "predictions_tree_1 = np.zeros((num_bootstrap_datasets, len(X_test)))\n",
    "predictions_tree_2 = np.zeros((num_bootstrap_datasets, len(X_test)))\n",
    "\n",
    "# Train the models on each of the 3000 bootstrapped datasets\n",
    "for i in range(num_bootstrap_datasets):\n",
    "    # Generate bootstrapped training data\n",
    "    X_bootstrap, y_bootstrap = bootstrap_dataset(X_train, y_train, N)\n",
    "\n",
    "    # Convert to numpy arrays to avoid warnings\n",
    "    X_bootstrap = X_bootstrap.to_numpy()\n",
    "    y_bootstrap = y_bootstrap.to_numpy()\n",
    "\n",
    "    \n",
    "    # Create a Bagging Regressor with 2 trees, each trained on 500 bootstrapped samples\n",
    "    bagging_model = BaggingRegressor(estimator=DecisionTreeRegressor(), \n",
    "                                     n_estimators=2, \n",
    "                                     max_samples=500, \n",
    "                                     bootstrap=True, \n",
    "                                     random_state=np.random.randint(1e6))\n",
    "    \n",
    "    # Fit the model\n",
    "    bagging_model.fit(X_bootstrap, y_bootstrap)\n",
    "    \n",
    "    # Get predictions for both trees\n",
    "    pred_trees = np.array([est.predict(X_test) for est in bagging_model.estimators_])\n",
    "    \n",
    "    # Store the predictions from each tree\n",
    "    predictions_tree_1[i, :] = pred_trees[0]  # Predictions from the first tree\n",
    "    predictions_tree_2[i, :] = pred_trees[1]  # Predictions from the second tree\n",
    "\n",
    "# Now compute the correlation for each test sample\n",
    "correlations = []\n",
    "for x_idx in range(len(X_test)):\n",
    "    # Get predictions for the current test sample (x) across all bootstrap datasets\n",
    "    preds_1 = predictions_tree_1[:, x_idx]\n",
    "    preds_2 = predictions_tree_2[:, x_idx]\n",
    "    \n",
    "    # Compute the correlation coefficient between the predictions of the two trees\n",
    "    corr_matrix = np.corrcoef(preds_1, preds_2)\n",
    "    corr = corr_matrix[0, 1]  # The off-diagonal element is the correlation coefficient\n",
    "    correlations.append(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations between tree predictions for each test point:\n",
      "0.04481161468025522\n"
     ]
    }
   ],
   "source": [
    "# Display the correlation results\n",
    "correlations = np.array(correlations)\n",
    "print(\"Correlations between tree predictions for each test point:\")\n",
    "print(np.mean(correlations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3B iii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average correlation for F = 2: 0.0268\n",
      "Average correlation for F = 5: 0.0360\n",
      "Average correlation for F = 7: 0.0415\n",
      "Average correlation for F = 9: 0.0426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "# Define the range of F values (number of features to consider at each split)\n",
    "F_values = [2, 5, 7, 9] \n",
    "\n",
    "# Store average correlations for each value of F\n",
    "average_correlations = {}\n",
    "\n",
    "# Iterate over each value of F\n",
    "for F in F_values:\n",
    "    # Initialize to store predictions for each tree on each bootstrap dataset\n",
    "    predictions_tree_1 = np.zeros((num_bootstrap_datasets, len(X_test)))\n",
    "    predictions_tree_2 = np.zeros((num_bootstrap_datasets, len(X_test)))\n",
    "\n",
    "    # Train the models on each of the 3000 bootstrapped datasets\n",
    "    for i in range(num_bootstrap_datasets):\n",
    "        # Generate bootstrapped training data\n",
    "        X_bootstrap, y_bootstrap = bootstrap_dataset(X_train, y_train, N)\n",
    "\n",
    "        # Convert to numpy arrays to avoid warnings\n",
    "        X_bootstrap = X_bootstrap.to_numpy()\n",
    "        y_bootstrap = y_bootstrap.to_numpy()\n",
    "\n",
    "        # Create a RandomForestRegressor with 2 trees, each trained on 500 bootstrapped samples and F features\n",
    "        random_forest_model = RandomForestRegressor(n_estimators=2,\n",
    "                                                    max_samples=500,\n",
    "                                                    max_features=F,\n",
    "                                                    bootstrap=True,\n",
    "                                                    random_state=np.random.randint(1e6))\n",
    "        \n",
    "        # Fit the model on the bootstrapped training data\n",
    "        random_forest_model.fit(X_bootstrap, y_bootstrap)\n",
    "\n",
    "        # Get predictions for both trees\n",
    "        pred_trees = np.array([est.predict(X_test) for est in random_forest_model.estimators_])\n",
    "        \n",
    "        # Store the predictions from each tree\n",
    "        predictions_tree_1[i, :] = pred_trees[0]  # Predictions from the first tree\n",
    "        predictions_tree_2[i, :] = pred_trees[1]  # Predictions from the second tree\n",
    "\n",
    "    # Compute the basic correlation for each test sample and average over all test samples\n",
    "    correlations = []\n",
    "    for x_idx in range(len(X_test)):\n",
    "        # Get predictions for the current test sample (x) across all bootstrap datasets\n",
    "        preds_1 = predictions_tree_1[:, x_idx]\n",
    "        preds_2 = predictions_tree_2[:, x_idx]\n",
    "        \n",
    "        # Compute the basic correlation coefficient between the predictions of the two trees\n",
    "        corr_matrix = np.corrcoef(preds_1, preds_2)\n",
    "        corr = corr_matrix[0, 1]  # The off-diagonal element is the correlation coefficient\n",
    "        correlations.append(corr)\n",
    "\n",
    "    # Compute the average correlation for the current value of F\n",
    "    average_correlations[F] = np.mean(correlations)\n",
    "\n",
    "# Display the average correlation results for each value of F\n",
    "for F, avg_corr in average_correlations.items():\n",
    "    print(f\"Average correlation for F = {F}: {avg_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3B iiii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: values will be different on different runs\n",
    "\n",
    "The tree bagging method had an average correlation of about 0.047 where as random forest method had:\n",
    "\n",
    "Average correlation for F = 2: 0.0267\n",
    "\n",
    "Average correlation for F = 5: 0.0383\n",
    "\n",
    "Average correlation for F = 7: 0.0386\n",
    "\n",
    "Average correlation for F = 9: 0.0442\n",
    "\n",
    "In makes sense that tree bagging method performed worse than random forest as the main point of doing the random forest was to reduce correlation.\n",
    "\n",
    "Clearly, as F increases correlation also increase. This makes sense. As F increases, more features are considered at each split, making it more likely that different trees will choose the same or similar features to split on. Small F's will naturally results in more different subsets of the available features for the two trees, which reduces correlation between their predictions. So yes this is about what I expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
