{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note: \n",
    "\n",
    "$\\nabla f(x)  = \\nabla x^2 = 2x$\n",
    "\n",
    "$x* = 0$ as that is the global/local min of $x^2$\n",
    "\n",
    "\\begin{align*}\n",
    "    x_t &= x_{t-1} - 0.1*(2*x_{t-1})\\\\\n",
    "    x_0 &= 1\\\\\n",
    "    x_1 &= 1 - 0.1*(2*1)= 0.8\\\\\n",
    "    x_2 &= 0.8 - 0.1*(2*0.8)= 0.64\\\\\n",
    "    x_3 &= 0.64 - 0.1*(2*0.64)= 0.512\\\\\n",
    "    \\text{ So } | x_3 - x* | &= 0.512 - 0 = 0.512\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 aii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align*}\n",
    "    v_t &= 0.5*v_{t-1} + 0.1*2*x_{t-1}\\\\\n",
    "    x_t &= x_{t-1} - v_t\\\\\n",
    "    v_0 &= 0\\\\\n",
    "    x_0 &= 1\\\\\n",
    "    v_1 &= 0.5*0 + 0.1*2*1 = 0.2\\\\\n",
    "    x_1 &= 1 - 0.2 = 0.8\\\\\n",
    "    v_2 &= 0.5*0.2 + 0.1*2*0.8= 0.26\\\\\n",
    "    x_2 &= 0.8 - 0.26 = 0.54\\\\\n",
    "    v_3 &= 0.5*0.26+ 0.1*2*0.54 = 0.238\\\\\n",
    "    x_3 &= 0.54 - 0.238 = 0.302\\\\\n",
    "    \\text{ So } | x_3 - x* | &= 0.302 - 0 = 0.302\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected gradient descent with momentum had the lower optimality gap after iteration 3 and thus converged faster. This makes sense as accelerated convergence is one of the main points of using gradient descent with momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Define the function and its gradient\n",
    "def f(x):\n",
    "    return np.sin(5*x) + (1/5) * x**2\n",
    "x = np.linspace(-np.pi*8, np.pi*8, 1000)\n",
    "x = np.linspace(-np.pi*2, np.pi*2, 1000)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, f(x), 'b-', label='f(x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Plot of F(x)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is challenging for gradient descent to optimize as there are many local minimum that gradient descent can get stuck in (depending on the starting point). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 bii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    return 5*np.cos(5*x) + (2/5) * x\n",
    "\n",
    "# Gradient Descent\n",
    "def gradient_descent(x0, eta, num_iterations):\n",
    "    x = x0\n",
    "    trajectory = [x]\n",
    "    for _ in range(num_iterations):\n",
    "        x = x - eta * grad_f(x)\n",
    "        trajectory.append(x)\n",
    "    return np.array(trajectory)\n",
    "\n",
    "x0 = 20\n",
    "eta = 0.1\n",
    "num_iterations = 20\n",
    "gd_trajectory = gradient_descent(x0, eta, num_iterations)\n",
    "\n",
    "x = np.linspace(-np.pi, np.pi*7, 1000)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, f(x), 'k-', label='f(x)')\n",
    "plt.plot(gd_trajectory, f(gd_trajectory), 'ro-', label='Gradient Descent', markersize=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent Trajectory')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note at iteration 9 ish Gradient Descent gets stuck at a local minimum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 biii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent with Momentum\n",
    "def gradient_descent_momentum(x0, eta, beta, num_iterations):\n",
    "    x = x0\n",
    "    v = 0 # just set it to 0 as I assumed no starting Momentum\n",
    "    trajectory = [x]\n",
    "    for _ in range(num_iterations):\n",
    "        v = beta * v + eta * grad_f(x)\n",
    "        x = x - v\n",
    "        trajectory.append(x)\n",
    "    return np.array(trajectory)\n",
    "\n",
    "x0 = 20\n",
    "eta = 0.1\n",
    "num_iterations = 20\n",
    "beta = 0.5\n",
    "gdm_trajectory = gradient_descent_momentum(x0, eta, beta, num_iterations)\n",
    "# Plotting\n",
    "x = np.linspace(-np.pi, np.pi*7, 1000)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, f(x), 'k-', label='f(x)')\n",
    "plt.plot(gdm_trajectory, f(gdm_trajectory), 'ro-', label='Gradient Descent with Momentum', markersize=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent with Momentum Trajectory')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the parameters Gradient Descent with Momentum almost achieves the optimum value (it still needs a few more iterations). In other words, Gradient Descent with Momentum will find the optimum value but Gradient Descent on its own will get stuck and never find it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 biv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 1\n",
    "eta = 0.1\n",
    "num_iterations = 200\n",
    "beta = 0.5\n",
    "gd_trajectory = gradient_descent(x0, eta, num_iterations)\n",
    "gdm_trajectory = gradient_descent_momentum(x0, eta, beta, num_iterations)\n",
    "\n",
    "# Plotting\n",
    "x = np.linspace(-np.pi/10, np.pi/2, 1000)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, f(x), 'k-', label='f(x)', linewidth=2)\n",
    "# Plot GD trajectory with slightly larger, semi-transparent markers\n",
    "plt.plot(gd_trajectory, f(gd_trajectory), 'b-', label='Gradient Descent', linewidth=1.5)\n",
    "plt.scatter(gd_trajectory, f(gd_trajectory), color='blue', s=40, alpha = 0.7)\n",
    "# Plot GDM trajectory with slightly smaller, fully opaque markers\n",
    "plt.plot(gdm_trajectory, f(gdm_trajectory), 'r-', label='Gradient Descent with Momentum', linewidth=1.5)\n",
    "plt.scatter(gdm_trajectory, f(gdm_trajectory), color='red', s=20, alpha=1)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent Trajectories Comparison Near a Minima')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above Gradient Descent with Momentum quickly adjust its step size and found the local minima rather quickly. Gradient Descent on the other hand jumped around from side to side slowly going down until it got stuck jumping between two points. \n",
    "\n",
    "It should also be noted that neither one was able to find the global minimum. Gradient Descent with Momentum did better starting further from the Global Minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2ci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second-order Taylor series expansion of L(w) around w0 is given by:\n",
    "\n",
    "$$\\bar{L}(w) = L(w_0) + \\nabla L(w_0)(w - w_0) + \\frac{1}{2}(w - w_0)^T H L(w_0)(w - w_0)$$\n",
    "Where:\n",
    "- L(w0) is the value of the function at the starting point w0\n",
    "- ∇L(w0) is the gradient of L at w0\n",
    "- HL(w0) is the Hessian matrix of L at w0\n",
    "\n",
    "Note: used this source to see what the second-order Taylor series expansion is. https://mathinsight.org/taylors_theorem_multivariable_introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2cii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   $$\\bar{L}(w) = L(w_0) + \\nabla L(w_0)(w - w_0) + \\frac{1}{2}(w - w_0)^T H L(w_0)(w - w_0)$$\n",
    "To minimize $\\bar{L}(w)$ we take its derivative and set it equal to 0.\n",
    "\n",
    "   $$\\bar{L}(w) = L(w_0) -w_0\\nabla L(w_0) + w\\nabla L(w_0) + \\frac{1}{2}(w - w_0)^T H L(w_0)(w - w_0)$$\n",
    "\n",
    "   Note here first to terms go to 0 when taking the derivate then the other two are shown in order\n",
    "\n",
    "   $$\\frac{\\partial \\bar{L}(w)}{\\partial w} = \\nabla L(w_0) + H L(w_0)(w - w_0) = 0$$\n",
    "\n",
    "Solve for w:\n",
    "\n",
    "   $$H L(w_0)(w - w_0) = -\\nabla L(w_0)$$\n",
    "   $$(w - w_0) = -[H L(w_0)]^{-1}\\nabla L(w_0)$$\n",
    "\n",
    "Therefore, the value of w that minimizes $\\bar{L}(w)$ is:\n",
    "\n",
    "   $$w_1 = w_0 - [H L(w_0)]^{-1}\\nabla L(w_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2ciii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(w) = \\frac{1}{2} w^T A w - b^T w$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given\n",
    "$$L(w) = \\frac{1}{2} w^T A w - b^T w$$\n",
    "\n",
    "The gradient of L(w) is:\n",
    "$$\\nabla L(w) = Aw - b$$\n",
    "\n",
    "The Hessian matrix is (as this a quadratic): \n",
    "$$\\nabla \\nabla L(w) = HL(w) =  A $$\n",
    "\n",
    "Now we can plug w1: \n",
    "   $$w_1 = w_0 - [H L(w_0)]^{-1}\\nabla L(w_0)$$\n",
    "   $$w_1 = w_0 - A^{-1} (Aw_0-b)$$\n",
    "   $$w_1 = w_0 - A^{-1}Aw_0-A^{-1}b$$\n",
    "   $$w_1 = w_0 - w_0-A^{-1}b$$\n",
    "   $$w_1 = A^{-1}b$$\n",
    "\n",
    "\n",
    "Note: For a quadratic function like L(w), the second-order Taylor expansion is exact (not an approximation).\n",
    "\n",
    "Now by setting the gradient to zero we see that the second-order method finds the exact minimizer in a single step, regardless of the starting point w0:\n",
    "   $$\\nabla L(w) = Aw - b = 0$$\n",
    "   $$Aw = b$$\n",
    "   $$w = A^{-1}b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2civ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-world scenarios with high-dimensional parameter spaces the second-order optimization method has two main bottlenecks\n",
    "\n",
    "### The main memory bottleneck is the storage of the Hessian matrix\n",
    "\n",
    "**Hessian Matrix Size**: For a model with n parameters, the Hessian matrix H is an n × n matrix. In a neural network with millions of parameters, this matrix becomes extremely large.\n",
    "\n",
    "### The main computational bottleneck is the inversion of the Hessian matrix:\n",
    "\n",
    " The time complexity of inverting an n × n matrix where n > million is really slow(assuming it is even invertible).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
